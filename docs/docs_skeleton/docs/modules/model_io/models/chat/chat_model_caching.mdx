# Caching
LangChain provides an optional caching layer for Chat Models. This is useful for two reasons:

It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.
It can speed up your application by reducing the number of API calls you make to the LLM provider.

import CachingChat from "@snippets/modules/model_io/models/chat/how_to/chat_model_caching.mdx"

<CachingChat/>
