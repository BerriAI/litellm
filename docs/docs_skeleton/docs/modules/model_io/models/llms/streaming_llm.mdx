# Streaming

Some LLMs provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it's available. This is useful if you want to display the response to the user as it's being generated, or if you want to process the response as it's being generated.

import StreamingLLM from "@snippets/modules/model_io/models/llms/how_to/streaming_llm.mdx"

<StreamingLLM/>
