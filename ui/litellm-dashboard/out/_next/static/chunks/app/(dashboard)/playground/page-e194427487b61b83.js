(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[3368],{52232:function(e,n,t){Promise.resolve().then(t.bind(t,33837))},58643:function(e,n,t){"use strict";t.d(n,{OK:function(){return i.Z},nP:function(){return s.Z},td:function(){return o.Z},v0:function(){return a.Z},x4:function(){return r.Z}});var i=t(12485),a=t(18135),o=t(35242),r=t(29706),s=t(77991)},92280:function(e,n,t){"use strict";t.d(n,{x:function(){return i.Z}});var i=t(84264)},39760:function(e,n,t){"use strict";var i=t(19250),a=t(3914),o=t(97060),r=t(99376),s=t(2265),l=t(76191);n.Z=()=>{var e,n,t,p,u;let c=(0,r.useRouter)(),{data:m,isLoading:d}=(0,l.p)(),g="undefined"!=typeof document?(0,a.e)("token"):null,_=(0,s.useMemo)(()=>(0,o.TD)(g),[g]),f=(0,s.useMemo)(()=>(0,o.sT)(g),[g])&&!(null==m?void 0:m.admin_ui_disabled);return(0,s.useEffect)(()=>{d||f||(g&&(0,a.b)(),c.replace("".concat((0,i.getProxyBaseUrl)(),"/ui/login")))},[d,f,g,c]),{isLoading:d,isAuthorized:f,token:f?g:null,accessToken:null!==(e=null==_?void 0:_.key)&&void 0!==e?e:null,userId:null!==(n=null==_?void 0:_.user_id)&&void 0!==n?n:null,userEmail:null!==(t=null==_?void 0:_.user_email)&&void 0!==t?t:null,userRole:function(e){if(!e)return"Undefined Role";switch(e.toLowerCase()){case"app_owner":case"demo_app_owner":return"App Owner";case"app_admin":case"proxy_admin":return"Admin";case"proxy_admin_viewer":return"Admin Viewer";case"org_admin":return"Org Admin";case"internal_user":return"Internal User";case"internal_user_viewer":case"internal_viewer":return"Internal Viewer";case"app_user":return"App User";default:return"Unknown Role"}}(null==_?void 0:_.user_role),premiumUser:null!==(p=null==_?void 0:_.premium_user)&&void 0!==p?p:null,disabledPersonalKeyCreation:null!==(u=null==_?void 0:_.disabled_non_admin_personal_key_creation)&&void 0!==u?u:null,showSSOBanner:(null==_?void 0:_.login_method)==="username_password"}}},67479:function(e,n,t){"use strict";var i=t(57437),a=t(2265),o=t(37592),r=t(19250);n.Z=e=>{let{onChange:n,value:t,className:s,accessToken:l,disabled:p}=e,[u,c]=(0,a.useState)([]),[m,d]=(0,a.useState)(!1);return(0,a.useEffect)(()=>{(async()=>{if(l){d(!0);try{let e=await (0,r.getGuardrailsList)(l);console.log("Guardrails response:",e),e.guardrails&&(console.log("Guardrails data:",e.guardrails),c(e.guardrails))}catch(e){console.error("Error fetching guardrails:",e)}finally{d(!1)}}})()},[l]),(0,i.jsx)("div",{children:(0,i.jsx)(o.default,{mode:"multiple",disabled:p,placeholder:p?"Setting guardrails is a premium feature.":"Select guardrails",onChange:e=>{console.log("Selected guardrails:",e),n(e)},value:t,loading:m,className:s,allowClear:!0,options:u.map(e=>(console.log("Mapping guardrail:",e),{label:"".concat(e.guardrail_name),value:e.guardrail_name})),optionFilterProp:"label",showSearch:!0,style:{width:"100%"}})})}},82971:function(e,n,t){"use strict";t.d(n,{L:function(){return a}});var i=t(8443);let a=e=>{let n;let{apiKeySource:t,accessToken:a,apiKey:o,inputMessage:r,chatHistory:s,selectedTags:l,selectedVectorStores:p,selectedGuardrails:u,selectedPolicies:c,selectedMCPServers:m,mcpServers:d,mcpServerToolRestrictions:g,selectedVoice:_,endpointType:f,selectedModel:h,selectedSdk:b,proxySettings:y}=e,v="session"===t?a:o,w=window.location.origin,x=null==y?void 0:y.LITELLM_UI_API_DOC_BASE_URL;x&&x.trim()?w=x:(null==y?void 0:y.PROXY_BASE_URL)&&(w=y.PROXY_BASE_URL);let E=r||"Your prompt here",I=E.replace(/\\/g,"\\\\").replace(/"/g,'\\"').replace(/\n/g,"\\n"),S=s.filter(e=>!e.isImage).map(e=>{let{role:n,content:t}=e;return{role:n,content:t}}),A={};l.length>0&&(A.tags=l),p.length>0&&(A.vector_stores=p),u.length>0&&(A.guardrails=u),c.length>0&&(A.policies=c);let P=h||"your-model-name",O="azure"===b?'import openai\n\nclient = openai.AzureOpenAI(\n	api_key="'.concat(v||"YOUR_LITELLM_API_KEY",'",\n	azure_endpoint="').concat(w,'",\n	api_version="2024-02-01"\n)'):'import openai\n\nclient = openai.OpenAI(\n	api_key="'.concat(v||"YOUR_LITELLM_API_KEY",'",\n	base_url="').concat(w,'"\n)');switch(f){case i.KP.CHAT:{let e=Object.keys(A).length>0,t="";if(e){let e=JSON.stringify({metadata:A},null,2).split("\n").map(e=>" ".repeat(4)+e).join("\n").trim();t=",\n    extra_body=".concat(e)}let i=S.length>0?S:[{role:"user",content:E}];n='\nimport base64\n\n# Helper function to encode images to base64\ndef encode_image(image_path):\n    with open(image_path, "rb") as image_file:\n        return base64.b64encode(image_file.read()).decode(\'utf-8\')\n\n# Example with text only\nresponse = client.chat.completions.create(\n    model="'.concat(P,'",\n    messages=').concat(JSON.stringify(i,null,4)).concat(t,'\n)\n\nprint(response)\n\n# Example with image or PDF (uncomment and provide file path to use)\n# base64_file = encode_image("path/to/your/file.jpg")  # or .pdf\n# response_with_file = client.chat.completions.create(\n#     model="').concat(P,'",\n#     messages=[\n#         {\n#             "role": "user",\n#             "content": [\n#                 {\n#                     "type": "text",\n#                     "text": "').concat(I,'"\n#                 },\n#                 {\n#                     "type": "image_url",\n#                     "image_url": {\n#                         "url": f"data:image/jpeg;base64,{base64_file}"  # or data:application/pdf;base64,{base64_file}\n#                     }\n#                 }\n#             ]\n#         }\n#     ]').concat(t,"\n# )\n# print(response_with_file)\n");break}case i.KP.RESPONSES:{let e=Object.keys(A).length>0,t="";if(e){let e=JSON.stringify({metadata:A},null,2).split("\n").map(e=>" ".repeat(4)+e).join("\n").trim();t=",\n    extra_body=".concat(e)}let i=S.length>0?S:[{role:"user",content:E}];n='\nimport base64\n\n# Helper function to encode images to base64\ndef encode_image(image_path):\n    with open(image_path, "rb") as image_file:\n        return base64.b64encode(image_file.read()).decode(\'utf-8\')\n\n# Example with text only\nresponse = client.responses.create(\n    model="'.concat(P,'",\n    input=').concat(JSON.stringify(i,null,4)).concat(t,'\n)\n\nprint(response.output_text)\n\n# Example with image or PDF (uncomment and provide file path to use)\n# base64_file = encode_image("path/to/your/file.jpg")  # or .pdf\n# response_with_file = client.responses.create(\n#     model="').concat(P,'",\n#     input=[\n#         {\n#             "role": "user",\n#             "content": [\n#                 {"type": "input_text", "text": "').concat(I,'"},\n#                 {\n#                     "type": "input_image",\n#                     "image_url": f"data:image/jpeg;base64,{base64_file}",  # or data:application/pdf;base64,{base64_file}\n#                 },\n#             ],\n#         }\n#     ]').concat(t,"\n# )\n# print(response_with_file.output_text)\n");break}case i.KP.IMAGE:n="azure"===b?"\n# NOTE: The Azure SDK does not have a direct equivalent to the multi-modal 'responses.create' method shown for OpenAI.\n# This snippet uses 'client.images.generate' and will create a new image based on your prompt.\n# It does not use the uploaded image, as 'client.images.generate' does not support image inputs in this context.\nimport os\nimport requests\nimport json\nimport time\nfrom PIL import Image\n\nresult = client.images.generate(\n	model=\"".concat(P,'",\n	prompt="').concat(r,'",\n	n=1\n)\n\njson_response = json.loads(result.model_dump_json())\n\n# Set the directory for the stored image\nimage_dir = os.path.join(os.curdir, \'images\')\n\n# If the directory doesn\'t exist, create it\nif not os.path.isdir(image_dir):\n	os.mkdir(image_dir)\n\n# Initialize the image path\nimage_filename = f"generated_image_{int(time.time())}.png"\nimage_path = os.path.join(image_dir, image_filename)\n\ntry:\n	# Retrieve the generated image\n	if json_response.get("data") && len(json_response["data"]) > 0 && json_response["data"][0].get("url"):\n			image_url = json_response["data"][0]["url"]\n			generated_image = requests.get(image_url).content\n			with open(image_path, "wb") as image_file:\n					image_file.write(generated_image)\n\n			print(f"Image saved to {image_path}")\n			# Display the image\n			image = Image.open(image_path)\n			image.show()\n	else:\n			print("Could not find image URL in response.")\n			print("Full response:", json_response)\nexcept Exception as e:\n	print(f"An error occurred: {e}")\n	print("Full response:", json_response)\n'):"\nimport base64\nimport os\nimport time\nimport json\nfrom PIL import Image\nimport requests\n\n# Helper function to encode images to base64\ndef encode_image(image_path):\n	with open(image_path, \"rb\") as image_file:\n			return base64.b64encode(image_file.read()).decode('utf-8')\n\n# Helper function to create a file (simplified for this example)\ndef create_file(image_path):\n	# In a real implementation, this would upload the file to OpenAI\n	# For this example, we'll just return a placeholder ID\n	return f\"file_{os.path.basename(image_path).replace('.', '_')}\"\n\n# The prompt entered by the user\nprompt = \"".concat(I,'"\n\n# Encode images to base64\nbase64_image1 = encode_image("body-lotion.png")\nbase64_image2 = encode_image("soap.png")\n\n# Create file IDs\nfile_id1 = create_file("body-lotion.png")\nfile_id2 = create_file("incense-kit.png")\n\nresponse = client.responses.create(\n	model="').concat(P,'",\n	input=[\n			{\n					"role": "user",\n					"content": [\n							{"type": "input_text", "text": prompt},\n							{\n									"type": "input_image",\n									"image_url": f"data:image/jpeg;base64,{base64_image1}",\n							},\n							{\n									"type": "input_image",\n									"image_url": f"data:image/jpeg;base64,{base64_image2}",\n							},\n							{\n									"type": "input_image",\n									"file_id": file_id1,\n							},\n							{\n									"type": "input_image",\n									"file_id": file_id2,\n							}\n					],\n			}\n	],\n	tools=[{"type": "image_generation"}],\n)\n\n# Process the response\nimage_generation_calls = [\n	output\n	for output in response.output\n	if output.type == "image_generation_call"\n]\n\nimage_data = [output.result for output in image_generation_calls]\n\nif image_data:\n	image_base64 = image_data[0]\n	image_filename = f"edited_image_{int(time.time())}.png"\n	with open(image_filename, "wb") as f:\n			f.write(base64.b64decode(image_base64))\n	print(f"Image saved to {image_filename}")\nelse:\n	# If no image is generated, there might be a text response with an explanation\n	text_response = [output.text for output in response.output if hasattr(output, \'text\')]\n	if text_response:\n			print("No image generated. Model response:")\n			print("\\n".join(text_response))\n	else:\n			print("No image data found in response.")\n	print("Full response for debugging:")\n	print(response)\n');break;case i.KP.IMAGE_EDITS:n="azure"===b?'\nimport base64\nimport os\nimport time\nimport json\nfrom PIL import Image\nimport requests\n\n# Helper function to encode images to base64\ndef encode_image(image_path):\n	with open(image_path, "rb") as image_file:\n			return base64.b64encode(image_file.read()).decode(\'utf-8\')\n\n# The prompt entered by the user\nprompt = "'.concat(I,'"\n\n# Encode images to base64\nbase64_image1 = encode_image("body-lotion.png")\nbase64_image2 = encode_image("soap.png")\n\n# Create file IDs\nfile_id1 = create_file("body-lotion.png")\nfile_id2 = create_file("incense-kit.png")\n\nresponse = client.responses.create(\n	model="').concat(P,'",\n	input=[\n			{\n					"role": "user",\n					"content": [\n							{"type": "input_text", "text": prompt},\n							{\n									"type": "input_image",\n									"image_url": f"data:image/jpeg;base64,{base64_image1}",\n							},\n							{\n									"type": "input_image",\n									"image_url": f"data:image/jpeg;base64,{base64_image2}",\n							},\n							{\n									"type": "input_image",\n									"file_id": file_id1,\n							},\n							{\n									"type": "input_image",\n									"file_id": file_id2,\n							}\n					],\n			}\n	],\n	tools=[{"type": "image_generation"}],\n)\n\n# Process the response\nimage_generation_calls = [\n	output\n	for output in response.output\n	if output.type == "image_generation_call"\n]\n\nimage_data = [output.result for output in image_generation_calls]\n\nif image_data:\n	image_base64 = image_data[0]\n	image_filename = f"edited_image_{int(time.time())}.png"\n	with open(image_filename, "wb") as f:\n			f.write(base64.b64decode(image_base64))\n	print(f"Image saved to {image_filename}")\nelse:\n	# If no image is generated, there might be a text response with an explanation\n	text_response = [output.text for output in response.output if hasattr(output, \'text\')]\n	if text_response:\n			print("No image generated. Model response:")\n			print("\\n".join(text_response))\n	else:\n			print("No image data found in response.")\n	print("Full response for debugging:")\n	print(response)\n'):"\nimport base64\nimport os\nimport time\n\n# Helper function to encode images to base64\ndef encode_image(image_path):\n	with open(image_path, \"rb\") as image_file:\n			return base64.b64encode(image_file.read()).decode('utf-8')\n\n# Helper function to create a file (simplified for this example)\ndef create_file(image_path):\n	# In a real implementation, this would upload the file to OpenAI\n	# For this example, we'll just return a placeholder ID\n	return f\"file_{os.path.basename(image_path).replace('.', '_')}\"\n\n# The prompt entered by the user\nprompt = \"".concat(I,'"\n\n# Encode images to base64\nbase64_image1 = encode_image("body-lotion.png")\nbase64_image2 = encode_image("soap.png")\n\n# Create file IDs\nfile_id1 = create_file("body-lotion.png")\nfile_id2 = create_file("incense-kit.png")\n\nresponse = client.responses.create(\n	model="').concat(P,'",\n	input=[\n			{\n					"role": "user",\n					"content": [\n							{"type": "input_text", "text": prompt},\n							{\n									"type": "input_image",\n									"image_url": f"data:image/jpeg;base64,{base64_image1}",\n							},\n							{\n									"type": "input_image",\n									"image_url": f"data:image/jpeg;base64,{base64_image2}",\n							},\n							{\n									"type": "input_image",\n									"file_id": file_id1,\n							},\n							{\n									"type": "input_image",\n									"file_id": file_id2,\n							}\n					],\n			}\n	],\n	tools=[{"type": "image_generation"}],\n)\n\n# Process the response\nimage_generation_calls = [\n	output\n	for output in response.output\n	if output.type == "image_generation_call"\n]\n\nimage_data = [output.result for output in image_generation_calls]\n\nif image_data:\n	image_base64 = image_data[0]\n	image_filename = f"edited_image_{int(time.time())}.png"\n	with open(image_filename, "wb") as f:\n			f.write(base64.b64decode(image_base64))\n	print(f"Image saved to {image_filename}")\nelse:\n	# If no image is generated, there might be a text response with an explanation\n	text_response = [output.text for output in response.output if hasattr(output, \'text\')]\n	if text_response:\n			print("No image generated. Model response:")\n			print("\\n".join(text_response))\n	else:\n			print("No image data found in response.")\n	print("Full response for debugging:")\n	print(response)\n');break;case i.KP.EMBEDDINGS:n='\nresponse = client.embeddings.create(\n	input="'.concat(r||"Your string here",'",\n	model="').concat(P,'",\n	encoding_format="base64" # or "float"\n)\n\nprint(response.data[0].embedding)\n');break;case i.KP.TRANSCRIPTION:n='\n# Open the audio file\naudio_file = open("path/to/your/audio/file.mp3", "rb")\n\n# Make the transcription request\nresponse = client.audio.transcriptions.create(\n	model="'.concat(P,'",\n	file=audio_file').concat(r?',\n	prompt="'.concat(r.replace(/"/g,'\\"'),'"'):"","\n)\n\nprint(response.text)\n");break;case i.KP.SPEECH:n='\n# Make the text-to-speech request\nresponse = client.audio.speech.create(\n	model="'.concat(P,'",\n	input="').concat(r||"Your text to convert to speech here",'",\n	voice="').concat(_,'"  # Options: alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, shimmer\n)\n\n# Save the audio to a file\noutput_filename = "output_speech.mp3"\nresponse.stream_to_file(output_filename)\nprint(f"Audio saved to {output_filename}")\n\n# Optional: Customize response format and speed\n# response = client.audio.speech.create(\n#     model="').concat(P,'",\n#     input="').concat(r||"Your text to convert to speech here",'",\n#     voice="alloy",\n#     response_format="mp3",  # Options: mp3, opus, aac, flac, wav, pcm\n#     speed=1.0  # Range: 0.25 to 4.0\n# )\n# response.stream_to_file("output_speech.mp3")\n');break;default:n="\n# Code generation for this endpoint is not implemented yet."}return"".concat(O,"\n").concat(n)}},8443:function(e,n,t){"use strict";var i,a,o,r;t.d(n,{KP:function(){return a},vf:function(){return l}}),(o=i||(i={})).AUDIO_SPEECH="audio_speech",o.AUDIO_TRANSCRIPTION="audio_transcription",o.IMAGE_GENERATION="image_generation",o.VIDEO_GENERATION="video_generation",o.CHAT="chat",o.RESPONSES="responses",o.IMAGE_EDITS="image_edits",o.ANTHROPIC_MESSAGES="anthropic_messages",o.EMBEDDING="embedding",(r=a||(a={})).IMAGE="image",r.VIDEO="video",r.CHAT="chat",r.RESPONSES="responses",r.IMAGE_EDITS="image_edits",r.ANTHROPIC_MESSAGES="anthropic_messages",r.EMBEDDINGS="embeddings",r.SPEECH="speech",r.TRANSCRIPTION="transcription",r.A2A_AGENTS="a2a_agents",r.MCP="mcp";let s={image_generation:"image",video_generation:"video",chat:"chat",responses:"responses",image_edits:"image_edits",anthropic_messages:"anthropic_messages",audio_speech:"speech",audio_transcription:"transcription",embedding:"embeddings"},l=e=>{if(console.log("getEndpointType:",e),Object.values(i).includes(e)){let n=s[e];return console.log("endpointType:",n),n}return"chat"}},10703:function(e,n,t){"use strict";t.d(n,{p:function(){return a}});var i=t(19250);let a=async e=>{try{let n=await (0,i.modelHubCall)(e);if(console.log("model_info:",n),(null==n?void 0:n.data.length)>0){let e=n.data.map(e=>({model_group:e.model_group,mode:null==e?void 0:e.mode}));return e.sort((e,n)=>e.model_group.localeCompare(n.model_group)),e}return[]}catch(e){throw console.error("Error fetching model info:",e),e}}},87972:function(e,n,t){"use strict";t.d(n,{oh:function(){return s}});var i=t(57437),a=t(2265),o=t(37592),r=t(19250);function s(e){return e.filter(e=>{var n;return(null!==(n=e.version_status)&&void 0!==n?n:"draft")!=="draft"}).map(e=>{var n,t,i;let a=null!==(n=e.version_number)&&void 0!==n?n:1,o=null!==(t=e.version_status)&&void 0!==t?t:"draft";return{label:"".concat(e.policy_name," — v").concat(a," (").concat(o,")").concat(e.description?" — ".concat(e.description):""),value:"production"===o?e.policy_name:e.policy_id?(i=e.policy_id,"".concat("policy_").concat(i)):e.policy_name}})}n.ZP=e=>{let{onChange:n,value:t,className:l,accessToken:p,disabled:u,onPoliciesLoaded:c}=e,[m,d]=(0,a.useState)([]),[g,_]=(0,a.useState)(!1);return(0,a.useEffect)(()=>{(async()=>{if(p){_(!0);try{let e=await (0,r.getPoliciesList)(p);e.policies&&(d(e.policies),null==c||c(e.policies))}catch(e){console.error("Error fetching policies:",e)}finally{_(!1)}}})()},[p,c]),(0,i.jsx)("div",{children:(0,i.jsx)(o.default,{mode:"multiple",disabled:u,placeholder:u?"Setting policies is a premium feature.":"Select policies (production or published versions)",onChange:e=>{n(e)},value:t,loading:g,className:l,allowClear:!0,options:s(m),optionFilterProp:"label",showSearch:!0,style:{width:"100%"}})})}},97415:function(e,n,t){"use strict";var i=t(57437),a=t(2265),o=t(37592),r=t(19250);n.Z=e=>{let{onChange:n,value:t,className:s,accessToken:l,placeholder:p="Select vector stores",disabled:u=!1}=e,[c,m]=(0,a.useState)([]),[d,g]=(0,a.useState)(!1);return(0,a.useEffect)(()=>{(async()=>{if(l){g(!0);try{let e=await (0,r.vectorStoreListCall)(l);e.data&&m(e.data)}catch(e){console.error("Error fetching vector stores:",e)}finally{g(!1)}}})()},[l]),(0,i.jsx)("div",{children:(0,i.jsx)(o.default,{mode:"multiple",placeholder:p,onChange:n,value:t,loading:d,className:s,allowClear:!0,options:c.map(e=>({label:"".concat(e.vector_store_name||e.vector_store_id," (").concat(e.vector_store_id,")"),value:e.vector_store_id,title:e.vector_store_description||e.vector_store_id})),optionFilterProp:"label",showSearch:!0,style:{width:"100%"},disabled:u})})}},97060:function(e,n,t){"use strict";t.d(n,{TD:function(){return o},sT:function(){return r},vf:function(){return a}});var i=t(14474);function a(e){try{let n=(0,i.o)(e);if(n&&"number"==typeof n.exp)return 1e3*n.exp<=Date.now();return!1}catch(e){return!0}}function o(e){if(!e)return null;try{return(0,i.o)(e)}catch(e){return null}}function r(e){return!!e&&null!==o(e)&&!a(e)}},91624:function(e,n,t){"use strict";t.d(n,{C:function(){return a}});var i=t(19250);let a=async e=>{if(!e)return null;try{return await (0,i.getProxyUISettings)(e)}catch(e){return console.error("Error fetching proxy settings:",e),null}}}},function(e){e.O(0,[7271,5173,7651,4865,7840,337,3367,5869,2652,2926,353,172,3709,6941,5319,7906,5986,4851,3560,1956,8921,1839,8765,6191,1953,7714,3837,2971,2117,1744],function(){return e(e.s=52232)}),_N_E=e.O()}]);