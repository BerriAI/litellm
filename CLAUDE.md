# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project: ClawRouter

ClawRouter is a smart model routing system built on top of LiteLLM. It routes requests to different tier models (low/mid/top) based on prompt content classification, helping users maximize their existing model access and credits by using expensive models only when needed.

### Quick Start
```bash
./setup.sh          # One-command setup: installs deps, prompts for API keys, configures tiers, starts proxy
```

### Manual Start
```bash
pip install -e .[proxy]
litellm --config litellm/proxy/proxy_config.yaml --port 4141
```

### Test It
```bash
# Health check
curl http://localhost:4141/health

# Auto-routed request
curl http://localhost:4141/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-1234" \
  -d '{"model":"auto","messages":[{"role":"user","content":"Hello!"}]}'
```

## Architecture

### Core Modules (`litellm/router_strategy/auto_router/`)

| File | Purpose |
|------|---------|
| `classifier.py` | Regex-based task classification engine (~50 patterns, 9 categories) |
| `tiers.py` | Model tier definitions (LOW/MID/TOP) and default category-to-tier mapping |
| `smart_router.py` | Core routing logic: classification, tier resolution, model selection, tier override tags |
| `auto_router.py` | LiteLLM integration layer: pre-routing hook, response tier prefixing, config loading |
| `routing_rules.yaml` | Runtime configuration: tier models, routing overrides, custom patterns |

### Configuration Files

| File | Purpose |
|------|---------|
| `litellm/proxy/proxy_config.yaml` | Proxy server config with auto-router model entry (gitignored, generated by setup.sh) |
| `litellm/router_strategy/auto_router/routing_rules.yaml` | Tier models, category-to-tier routing, custom regex patterns |
| `models.yaml` | Model metadata for setup.sh: tier candidates and provider model registry |
| `setup.sh` | One-command setup: Python check, venv, install, API key prompts, tier suggestions, proxy start |
| `.env` | API keys (gitignored, generated by setup.sh) |

### OpenClaw Plugin (`openclaw-plugin/`)

| File | Purpose |
|------|---------|
| `package.json` | npm package metadata with `openclaw.extensions` |
| `openclaw.plugin.json` | Plugin manifest: id, configSchema (port, masterKey, gitRepo), uiHints |
| `index.ts` | Entry point: reads pluginConfig, registers service + provider |
| `src/service.ts` | Proxy lifecycle: clone repo, venv, install, generate configs, start/stop |
| `src/provider.ts` | Provider registration: configPatch adds clawrouter provider + auto model |
| `src/keys.ts` | OpenClaw auth profile → LiteLLM env var mapping |

### Tests

| File | Purpose |
|------|---------|
| `tests/test_litellm/router_strategy/test_auto_router.py` | Full test suite: classification, tiers, routing, overrides, response prefixing |

### LiteLLM Integration Points
- `litellm/router.py` — Imports `AutoRouter`, maintains `auto_routers` dict, calls pre-routing hook
- `litellm/types/router.py` — `LiteLLM_Params` fields: `auto_router_config_path`, `auto_router_config`, `auto_router_default_model`

## Routing Flow

```
Request (model="auto") → Extract last user message
  → Check for tier override tag ([low]/[med]/[high])
    → If found: use forced tier, strip tag from message
    → If not: classify message with regex patterns → map category to tier
  → Select model for tier from config
  → Execute request on selected model
  → Prefix response with tier tag ("[low] ...", "[med] ...", "[high] ...")
  → Return to client
```

## Task Categories & Tier Mapping

| Category | Tier | Example Triggers |
|----------|------|------------------|
| HEARTBEAT | LOW | "hi", "ping", empty messages |
| SIMPLE_CHAT | LOW | Short casual messages (< 80 chars) |
| LOOKUP | LOW | "What is...", "Who is...", definitions |
| TRANSLATION | MID | "translate", language pairs |
| SUMMARIZATION | MID | "summarize", "tldr", "summary of" |
| CODING | MID | Code blocks, git commands, debugging keywords |
| CREATIVE | MID | "write a story", brainstorming, poetry |
| REASONING | TOP | "prove that", math, multi-step logic |
| ANALYSIS | TOP | "compare", "evaluate", pros/cons |

## Default Tier Models

| Tier | Default Model | Max Cost |
|------|--------------|----------|
| LOW | `deepseek/deepseek-chat-v3-0324` | $1.0/M tokens |
| MID | `zai/glm-4.7` | $5.0/M tokens |
| TOP | `anthropic/claude-sonnet-4-5` | $30.0/M tokens |

These are overridden by `routing_rules.yaml` at runtime.

## Key Features

### Tier Override Tags
Users can prefix messages with `[low]`, `[med]`/`[medium]`, or `[high]` to force a specific tier. The tag is stripped before sending to the model.

### Response Tier Prefixing
All responses are prefixed with the tier that handled them (`[low]`, `[med]`, `[high]`). Works for both streaming and non-streaming responses.

### Custom Regex Patterns
Add domain-specific classification rules in `routing_rules.yaml`:
```yaml
custom_patterns:
  - pattern: "kubernetes|k8s"
    category: "coding"
```
Custom patterns are evaluated before built-in patterns (first match wins).

### Model Normalization
`smart_router.py` normalizes bare model names to `provider/model` format (e.g., `claude-opus-4-6` → `anthropic/claude-opus-4`).

### Zero External Dependencies
Classification uses pure regex — no ML models, no semantic embeddings, millisecond latency.

## Development Commands

### Testing
```bash
# Run auto-router tests
poetry run pytest tests/test_litellm/router_strategy/test_auto_router.py -v

# Run all unit tests
make test-unit

# Run all tests
make test
```

### Code Quality
- `make lint` — Ruff, MyPy, Black, circular imports
- `make format` — Apply Black formatting

### Code Style
- Black formatter, Ruff linter, MyPy type checker
- Pydantic v2 for data validation
- Async/await patterns throughout
- Place imports at module level (not inside methods)

## routing_rules.yaml Format

```yaml
tiers:
  low:
    model: "gemini/gemini-2.5-flash-lite"
    max_cost_per_m_tokens: 1.0
  mid:
    model: "anthropic/claude-haiku-4-5-20251001"
    max_cost_per_m_tokens: 5.0
  top:
    model: "anthropic/claude-sonnet-4-5-20250929"
    max_cost_per_m_tokens: 30.0

routing:
  heartbeat: low
  simple-chat: low
  lookup: low
  translation: mid
  summarization: mid
  coding: mid
  creative: mid
  reasoning: top
  analysis: top

custom_patterns: []
```

## proxy_config.yaml Auto-Router Entry

```yaml
model_list:
  - model_name: auto
    litellm_params:
      model: "auto_router/auto_router_1"
      auto_router_config_path: "/absolute/path/to/routing_rules.yaml"
      auto_router_default_model: "anthropic/claude-sonnet-4-5"
```

## setup.sh Workflow

1. Find Python 3.10-3.13
2. Create/activate `.venv/`
3. `pip install -e .[proxy]`
4. Patch `proxy_config.yaml` with absolute path to `routing_rules.yaml`
5. Prompt for API keys (Google, OpenAI, Anthropic + optional providers) → save to `.env`
6. Suggest tier models from `models.yaml` based on available API keys
7. Auto-register provider models in `proxy_config.yaml`
8. Ensure "auto" model entry exists
9. Start proxy on port 4141

## OpenClaw Plugin

The `openclaw-plugin/` directory packages ClawRouter as an installable OpenClaw plugin. It automates the entire setup — no `setup.sh` or manual config needed.

### Install & Use
```bash
openclaw plugins install -l ./openclaw-plugin
# Restart gateway — service clones repo, installs deps, starts proxy
# clawrouter/auto is registered as the default model
```

### Plugin Flow
1. `register()` reads pluginConfig (port, masterKey, gitRepo)
2. `registerService()` → `start()`: clone repo → venv → pip install → extract API keys from OpenClaw auth profiles → generate configs → start proxy → health check
3. `registerProvider()` → configPatch adds `clawrouter` provider with `auto` model
4. User sends messages → routed through `clawrouter/auto` → classified → tier model → response with tier prefix

### Config (`openclaw.plugin.json`)
- `port` (default: 4141) — proxy port
- `masterKey` (default: "sk-clawrouter") — proxy auth key
- `gitRepo` — ClawRouter repo URL to clone
