{
  "longer_context_model_fallback_dict": {
	// openai chat completion models
	"gpt-3.5-turbo": "gpt-3.5-turbo-16k",
	"gpt-3.5-turbo-0301": "gpt-3.5-turbo-16k-0301",
	"gpt-3.5-turbo-0613": "gpt-3.5-turbo-16k-0613",
	"gpt-4": "gpt-4-32k",
	"gpt-4-0314": "gpt-4-32k-0314",
	"gpt-4-0613": "gpt-4-32k-0613",
	// anthropic
	"claude-instant-1": "claude-2",
	"claude-instant-1.2": "claude-2",
	// vertexai
	"chat-bison": "chat-bison-32k",
	"chat-bison@001": "chat-bison-32k",
	"codechat-bison": "codechat-bison-32k",
	"codechat-bison@001": "codechat-bison-32k",
	// openrouter
	"openrouter/openai/gpt-3.5-turbo": "openrouter/openai/gpt-3.5-turbo-16k",
	"openrouter/anthropic/claude-instant-v1": "openrouter/anthropic/claude-2",
  },
  "replicate_models": [
	// llama replicate supported LLMs
	"replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf",
	"a16z-infra/llama-2-13b-chat:2a7f981751ec7fdf87b5b91ad4db53683a98082e9ff7bfd12c8cd5ea85980a52",
	"meta/codellama-13b:1c914d844307b0588599b8393480a3ba917b660c7e9dfae681542b5325f228db",
	// Vicuna
	"replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b",
	"joehoover/instructblip-vicuna13b:c4c54e3c8c97cd50c2d2fec9be3b6065563ccf7d43787fb99f84151b867178fe",
	// Flan T-5
	"daanelson/flan-t5-large:ce962b3f6792a57074a601d3979db5839697add2e4e02696b3ced4c022d4767f",
	// Others
	"replicate/dolly-v2-12b:ef0e1aefc61f8e096ebe4db6b2bacc297daf2ef6899f0f7e001ec445893500e5",
	"replit/replit-code-v1-3b:b84f4c074b807211cd75e3e8b1589b6399052125b4c27106e43d47189e8415ad",
  ],
  "huggingface_models": [
	"meta-llama/Llama-2-7b-hf",
	"meta-llama/Llama-2-7b-chat-hf",
	"meta-llama/Llama-2-13b-hf",
	"meta-llama/Llama-2-13b-chat-hf",
	"meta-llama/Llama-2-70b-hf",
	"meta-llama/Llama-2-70b-chat-hf",
	"meta-llama/Llama-2-7b",
	"meta-llama/Llama-2-7b-chat",
	"meta-llama/Llama-2-13b",
	"meta-llama/Llama-2-13b-chat",
	"meta-llama/Llama-2-70b",
	"meta-llama/Llama-2-70b-chat",
	//these have been tested on extensively. But by default all text2text-generation and text-generation models are supported by liteLLM. - https://docs.litellm.ai/docs/providers
  ],
  "together_ai_models:": [
	// llama llms - chat
	"togethercomputer/llama-2-70b-chat",
	// llama llms - language / instruct
	"togethercomputer/llama-2-70b",
	"togethercomputer/LLaMA-2-7B-32K",
	"togethercomputer/Llama-2-7B-32K-Instruct",
	"togethercomputer/llama-2-7b",
	// falcon llms
	"togethercomputer/falcon-40b-instruct",
	"togethercomputer/falcon-7b-instruct",
	// alpaca
	"togethercomputer/alpaca-7b",
	// chat llms
	"HuggingFaceH4/starchat-alpha",
	// code llms
	"togethercomputer/CodeLlama-34b",
	"togethercomputer/CodeLlama-34b-Instruct",
	"togethercomputer/CodeLlama-34b-Python",
	"defog/sqlcoder",
	"NumbersStation/nsql-llama-2-7B",
	"WizardLM/WizardCoder-15B-V1.0",
	"WizardLM/WizardCoder-Python-34B-V1.0",
	// language llms
	"NousResearch/Nous-Hermes-Llama2-13b",
	"Austism/chronos-hermes-13b",
	"upstage/SOLAR-0-70b-16bit",
	"WizardLM/WizardLM-70B-V1.0",
	//supports all together ai models, just pass in the model id e.g. completion(model="together_computer/replit_code_3b",...)
  ],
  "baseten_models": [
	"qvv0xeq",
	"q841o8w",
	"31dxrj3",
	//# FALCON 7B  # WizardLM  # Mosaic ML
  ],
  "petals_models": [
	"petals-team/StableBeluga2",
  ],
  "ollama_models": [
	"llama2"
  ],
  "maritalk_models": [
	"maritalk"
  ],
  "provider": [
	"openai",
	"custom_openai",
	"cohere",
	"anthropic",
	"replicate",
	"huggingface",
	"together_ai",
	"openrouter",
	"vertex_ai",
	"palm",
	"ai21",
	"baseten",
	"azure",
	"sagemaker",
	"bedrock",
	"vllm",
	"nlp_cloud",
	"petals",
	"oobabooga",
	"ollama",
	"deepinfra",
	"perplexity",
	"anyscale",
	"maritalk",
	"custom",
	// custom apis
  ],
  // known openai compatible endpoints - we'll eventually move this list to the model_prices_and_context_window.json dictionary
  "openai_compatible": ["api.perplexity.ai"]
}