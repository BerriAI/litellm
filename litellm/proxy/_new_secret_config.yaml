# model_list: 
#   - model_name: my-fake-model
#     litellm_params:
#       model: bedrock/anthropic.claude-3-sonnet-20240229-v1:0
#       api_key: my-fake-key
#       aws_bedrock_runtime_endpoint: http://127.0.0.1:8000
#       mock_response: "Hello world 1"
#     model_info: 
#       max_input_tokens: 0 # trigger context window fallback
#   - model_name: my-fake-model
#     litellm_params:
#       model: bedrock/anthropic.claude-3-sonnet-20240229-v1:0
#       api_key: my-fake-key
#       aws_bedrock_runtime_endpoint: http://127.0.0.1:8000
#       mock_response: "Hello world 2"
#     model_info: 
#       max_input_tokens: 0

# router_settings:
#   enable_pre_call_checks: True


# litellm_settings:
#   failure_callback: ["langfuse"]

model_list:
  - model_name: summarize
    litellm_params:
        model: openai/gpt-4o
        rpm: 10000      
        tpm: 12000000
        api_key: os.environ/OPENAI_API_KEY
        mock_response: Hello world 1

  - model_name: summarize-l
    litellm_params:
        model: claude-3-5-sonnet-20240620
        rpm: 4000
        tpm: 400000
        api_key: os.environ/ANTHROPIC_API_KEY
        mock_response: Hello world 2

litellm_settings:
  num_retries: 3
  request_timeout: 120
  allowed_fails: 3
  # fallbacks: [{"summarize": ["summarize-l", "summarize-xl"]}, {"summarize-l": ["summarize-xl"]}]
  # context_window_fallbacks: [{"summarize": ["summarize-l", "summarize-xl"]}, {"summarize-l": ["summarize-xl"]}]



router_settings:
  routing_strategy: simple-shuffle
  enable_pre_call_checks: true.
