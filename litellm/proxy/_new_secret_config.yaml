model_list:
  - model_name: claude-3-5-sonnet-20240620
    litellm_params:
      model: claude-3-5-sonnet-20240620
      api_key: os.environ/ANTHROPIC_API_KEY
  - model_name: claude-3-5-sonnet-aihubmix
    litellm_params:
      model: openai/claude-3-5-sonnet-20240620
      input_cost_per_token: 0.000003 # 3$/M
      output_cost_per_token: 0.000015 # 15$/M
      api_base: "https://exampleopenaiendpoint-production.up.railway.app"
      api_key: my-fake-key
  - model_name: fake-openai-endpoint-2
    litellm_params:
      model: openai/my-fake-model
      api_key: my-fake-key
      api_base: https://exampleopenaiendpoint-production.up.railway.app/
      stream_timeout: 0.001
      timeout: 1
      rpm: 1
  - model_name: fake-openai-endpoint
    litellm_params:
      model: openai/my-fake-model
      api_key: my-fake-key
      api_base: https://exampleopenaiendpoint-production.up.railway.app/


# litellm_settings:
#   fallbacks: [{ "claude-3-5-sonnet-20240620": ["claude-3-5-sonnet-aihubmix"] }]
#   callbacks: ["otel", "prometheus"]
#   default_redis_batch_cache_expiry: 10


# litellm_settings:
#   cache: True
#   cache_params:
#     type: redis

#     # disable caching on the actual API call
#     supported_call_types: []

#     # see https://docs.litellm.ai/docs/proxy/prod#3-use-redis-porthost-password-not-redis_url
#     host: os.environ/REDIS_HOST
#     port: os.environ/REDIS_PORT
#     password: os.environ/REDIS_PASSWORD

#   # see https://docs.litellm.ai/docs/proxy/caching#turn-on-batch_redis_requests
#   # see https://docs.litellm.ai/docs/proxy/prometheus
#   callbacks: ['otel']


# # router_settings:
# #   routing_strategy: latency-based-routing
# #   routing_strategy_args:
# #     # only assign 40% of traffic to the fastest deployment to avoid overloading it
# #     lowest_latency_buffer: 0.4

# #     # consider last five minutes of calls for latency calculation
# #     ttl: 300
# #   redis_host: os.environ/REDIS_HOST
# #   redis_port: os.environ/REDIS_PORT
# #   redis_password: os.environ/REDIS_PASSWORD
  
# # # see https://docs.litellm.ai/docs/proxy/prod#1-use-this-configyaml
# # general_settings:
# #   master_key: os.environ/LITELLM_MASTER_KEY
# #   database_url: os.environ/DATABASE_URL
# #   disable_master_key_return: true
# #   # alerting: ['slack', 'email']
# #   alerting: ['email']

# #   # Batch write spend updates every 60s
# #   proxy_batch_write_at: 60

# #   # see https://docs.litellm.ai/docs/proxy/caching#advanced---user-api-key-cache-ttl
# #   # our api keys rarely change
# #   user_api_key_cache_ttl: 3600