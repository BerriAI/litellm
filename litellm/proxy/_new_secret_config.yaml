model_list:
  - model_name: openai/gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo
    model_info: 
      version: 2

router_settings:
  model_group_alias: {"gemini-2.5-pro": "openai/gpt-3.5-turbo"}

# Tried adding the below settings as well
litellm_settings:
  stream: false