model_list:
  - model_name: "*"
    litellm_params:
      model: claude-3-5-sonnet-20240620
      api_key: os.environ/ANTHROPIC_API_KEY
  - model_name: claude-3-5-sonnet-aihubmix
    litellm_params:
      model: openai/claude-3-5-sonnet-20240620
      input_cost_per_token: 0.000003 # 3$/M
      output_cost_per_token: 0.000015 # 15$/M
      api_base: "https://exampleopenaiendpoint-production.up.railway.app"
      api_key: my-fake-key
  - model_name: fake-openai-endpoint-2
    litellm_params:
      model: openai/my-fake-model
      api_key: my-fake-key
      api_base: https://exampleopenaiendpoint-production.up.railway.app/
      stream_timeout: 0.001
      timeout: 1
      rpm: 1
  - model_name: fake-openai-endpoint
    litellm_params:
      model: openai/my-fake-model
      api_key: my-fake-key
      api_base: https://exampleopenaiendpoint-production.up.railway.app/
  ## bedrock chat completions
  - model_name: "*anthropic.claude*"
    litellm_params:
      model: bedrock/*anthropic.claude*
      aws_access_key_id: os.environ/BEDROCK_AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/BEDROCK_AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
      guardrailConfig:
        "guardrailIdentifier": "h4dsqwhp6j66"
        "guardrailVersion": "2"
        "trace": "enabled"
        
## bedrock embeddings
  - model_name: "*amazon.titan-embed-*"
    litellm_params:
      model: bedrock/amazon.titan-embed-*
      aws_access_key_id: os.environ/BEDROCK_AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/BEDROCK_AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
  - model_name: "*cohere.embed-*"
    litellm_params:
      model: bedrock/cohere.embed-*
      aws_access_key_id: os.environ/BEDROCK_AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/BEDROCK_AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
  
  - model_name: "bedrock/*"
    litellm_params:
      model: bedrock/*
      aws_access_key_id: os.environ/BEDROCK_AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/BEDROCK_AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME

  - model_name: gpt-4
    litellm_params:
      model: azure/chatgpt-v-2
      api_base: https://openai-gpt-4-test-v-1.openai.azure.com/
      api_version: "2023-05-15"
      api_key: os.environ/AZURE_API_KEY # The `os.environ/` prefix tells litellm to read this from the env. See https://docs.litellm.ai/docs/simple_proxy#load-api-keys-from-vault
      rpm: 480
      timeout: 300
      stream_timeout: 60

litellm_settings:
  fallbacks: [{ "claude-3-5-sonnet-20240620": ["claude-3-5-sonnet-aihubmix"] }]
  callbacks: ["otel", "prometheus"]
  default_redis_batch_cache_expiry: 10
  # default_team_settings: 
  #   - team_id: "dbe2f686-a686-4896-864a-4c3924458709"
  #     success_callback: ["langfuse"]
  #     langfuse_public_key: os.environ/LANGFUSE_PUB_KEY_1 # Project 1
  #     langfuse_secret: os.environ/LANGFUSE_PRIVATE_KEY_1 # Project 1

# litellm_settings:
#   cache: True
#   cache_params:
#     type: redis

#     # disable caching on the actual API call
#     supported_call_types: []

#     # see https://docs.litellm.ai/docs/proxy/prod#3-use-redis-porthost-password-not-redis_url
#     host: os.environ/REDIS_HOST
#     port: os.environ/REDIS_PORT
#     password: os.environ/REDIS_PASSWORD

#   # see https://docs.litellm.ai/docs/proxy/caching#turn-on-batch_redis_requests
#   # see https://docs.litellm.ai/docs/proxy/prometheus
#   callbacks: ['otel']


# # router_settings:
# #   routing_strategy: latency-based-routing
# #   routing_strategy_args:
# #     # only assign 40% of traffic to the fastest deployment to avoid overloading it
# #     lowest_latency_buffer: 0.4

# #     # consider last five minutes of calls for latency calculation
# #     ttl: 300
# #   redis_host: os.environ/REDIS_HOST
# #   redis_port: os.environ/REDIS_PORT
# #   redis_password: os.environ/REDIS_PASSWORD
  
# # # see https://docs.litellm.ai/docs/proxy/prod#1-use-this-configyaml
# # general_settings:
# #   master_key: os.environ/LITELLM_MASTER_KEY
# #   database_url: os.environ/DATABASE_URL
# #   disable_master_key_return: true
# #   # alerting: ['slack', 'email']
# #   alerting: ['email']

# #   # Batch write spend updates every 60s
# #   proxy_batch_write_at: 60

# #   # see https://docs.litellm.ai/docs/proxy/caching#advanced---user-api-key-cache-ttl
# #   # our api keys rarely change
# #   user_api_key_cache_ttl: 3600
