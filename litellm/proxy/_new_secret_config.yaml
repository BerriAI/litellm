model_list:
  - model_name: gpt-4o
    litellm_params:
      model: openai/fake
      api_key: fake-key
      api_base: https://exampleopenaiendpoint-production.up.railway.app/

litellm_settings:
  callbacks: ["prometheus", "otel"]

general_settings:
  user_api_key_cache_ttl: 3600

router_settings:
  routing_strategy: latency-based-routing
  routing_strategy_args:
    # only assign 40% of traffic to the fastest deployment to avoid overloading it
    lowest_latency_buffer: 0.4

    # consider last five minutes of calls for latency calculation
    ttl: 300

  # model_group_alias:
  #   gpt-4o: gpt-4o-128k-2024-05-13
  #   gpt-4o-mini: gpt-4o-mini-128k-2024-07-18

  enable_tag_filtering: True

  # retry call 3 times on each model_name (we don't use fallbacks, so this would be 3 times total)
  num_retries: 3

  # -- cooldown settings --
  # see https://github.com/BerriAI/litellm/blob/main/litellm/router_utils/cooldown_handlers.py#L265

  # cooldown model if it fails > n calls in a minute.
  allowed_fails: 2

  # (in seconds) how long to cooldown model if fails/min > allowed_fails
  cooldown_time: 60

  allowed_fails_policy:
    InternalServerErrorAllowedFails: 1
    RateLimitErrorAllowedFails: 2
    TimeoutErrorAllowedFails: 3
  # -- end cooldown settings --

  # see https://docs.litellm.ai/docs/proxy/prod#3-use-redis-porthost-password-not-redis_url
  redis_host: os.environ/REDIS_HOST
  redis_port: os.environ/REDIS_PORT
  redis_password: os.environ/REDIS_PASSWORD
