(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[2322],{35831:function(e,n,t){Promise.resolve().then(t.bind(t,38511))},77565:function(e,n,t){"use strict";t.d(n,{Z:function(){return s}});var a=t(1119),i=t(2265),o={icon:{tag:"svg",attrs:{viewBox:"64 64 896 896",focusable:"false"},children:[{tag:"path",attrs:{d:"M765.7 486.8L314.9 134.7A7.97 7.97 0 00302 141v77.3c0 4.9 2.3 9.6 6.1 12.6l360 281.1-360 281.1c-3.9 3-6.1 7.7-6.1 12.6V883c0 6.7 7.7 10.4 12.9 6.3l450.8-352.1a31.96 31.96 0 000-50.4z"}}]},name:"right",theme:"outlined"},r=t(55015),s=i.forwardRef(function(e,n){return i.createElement(r.Z,(0,a.Z)({},e,{ref:n,icon:o}))})},69993:function(e,n,t){"use strict";t.d(n,{Z:function(){return s}});var a=t(1119),i=t(2265),o={icon:{tag:"svg",attrs:{viewBox:"64 64 896 896",focusable:"false"},children:[{tag:"path",attrs:{d:"M300 328a60 60 0 10120 0 60 60 0 10-120 0zM852 64H172c-17.7 0-32 14.3-32 32v660c0 17.7 14.3 32 32 32h680c17.7 0 32-14.3 32-32V96c0-17.7-14.3-32-32-32zm-32 660H204V128h616v596zM604 328a60 60 0 10120 0 60 60 0 10-120 0zm250.2 556H169.8c-16.5 0-29.8 14.3-29.8 32v36c0 4.4 3.3 8 7.4 8h729.1c4.1 0 7.4-3.6 7.4-8v-36c.1-17.7-13.2-32-29.7-32zM664 508H360c-4.4 0-8 3.6-8 8v60c0 4.4 3.6 8 8 8h304c4.4 0 8-3.6 8-8v-60c0-4.4-3.6-8-8-8z"}}]},name:"robot",theme:"outlined"},r=t(55015),s=i.forwardRef(function(e,n){return i.createElement(r.Z,(0,a.Z)({},e,{ref:n,icon:o}))})},57400:function(e,n,t){"use strict";t.d(n,{Z:function(){return s}});var a=t(1119),i=t(2265),o={icon:{tag:"svg",attrs:{viewBox:"0 0 1024 1024",focusable:"false"},children:[{tag:"path",attrs:{d:"M512 64L128 192v384c0 212.1 171.9 384 384 384s384-171.9 384-384V192L512 64zm312 512c0 172.3-139.7 312-312 312S200 748.3 200 576V246l312-110 312 110v330z"}},{tag:"path",attrs:{d:"M378.4 475.1a35.91 35.91 0 00-50.9 0 35.91 35.91 0 000 50.9l129.4 129.4 2.1 2.1a33.98 33.98 0 0048.1 0L730.6 434a33.98 33.98 0 000-48.1l-2.8-2.8a33.98 33.98 0 00-48.1 0L483 579.7 378.4 475.1z"}}]},name:"safety",theme:"outlined"},r=t(55015),s=i.forwardRef(function(e,n){return i.createElement(r.Z,(0,a.Z)({},e,{ref:n,icon:o}))})},15883:function(e,n,t){"use strict";t.d(n,{Z:function(){return s}});var a=t(1119),i=t(2265),o={icon:{tag:"svg",attrs:{viewBox:"64 64 896 896",focusable:"false"},children:[{tag:"path",attrs:{d:"M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"}}]},name:"user",theme:"outlined"},r=t(55015),s=i.forwardRef(function(e,n){return i.createElement(r.Z,(0,a.Z)({},e,{ref:n,icon:o}))})},96761:function(e,n,t){"use strict";t.d(n,{Z:function(){return l}});var a=t(5853),i=t(26898),o=t(97324),r=t(1153),s=t(2265);let l=s.forwardRef((e,n)=>{let{color:t,children:l,className:p}=e,m=(0,a._T)(e,["color","children","className"]);return s.createElement("p",Object.assign({ref:n,className:(0,o.q)("font-medium text-tremor-title",t?(0,r.bM)(t,i.K.darkText).textColor:"text-tremor-content-emphasis dark:text-dark-tremor-content-emphasis",p)},m),l)});l.displayName="Title"},92280:function(e,n,t){"use strict";t.d(n,{x:function(){return a.Z}});var a=t(84264)},39760:function(e,n,t){"use strict";var a=t(2265),i=t(99376),o=t(14474),r=t(3914);n.Z=()=>{var e,n,t,s,l,p,m;let c=(0,i.useRouter)(),u="undefined"!=typeof document?(0,r.e)("token"):null;(0,a.useEffect)(()=>{u||c.replace("/sso/key/generate")},[u,c]);let g=(0,a.useMemo)(()=>{if(!u)return null;try{return(0,o.o)(u)}catch(e){return(0,r.b)(),c.replace("/sso/key/generate"),null}},[u,c]);return{token:u,accessToken:null!==(e=null==g?void 0:g.key)&&void 0!==e?e:null,userId:null!==(n=null==g?void 0:g.user_id)&&void 0!==n?n:null,userEmail:null!==(t=null==g?void 0:g.user_email)&&void 0!==t?t:null,userRole:null!==(s=null==g?void 0:g.user_role)&&void 0!==s?s:null,premiumUser:null!==(l=null==g?void 0:g.premium_user)&&void 0!==l?l:null,disabledPersonalKeyCreation:null!==(p=null==g?void 0:g.disabled_non_admin_personal_key_creation)&&void 0!==p?p:null,showSSOBanner:(null==g?void 0:g.login_method)==="username_password"}}},38511:function(e,n,t){"use strict";t.r(n);var a=t(57437),i=t(31052),o=t(39760);n.default=()=>{let{token:e,accessToken:n,userRole:t,userId:r,disabledPersonalKeyCreation:s}=(0,o.Z)();return(0,a.jsx)(i.Z,{accessToken:n,token:e,userRole:t,userID:r,disabledPersonalKeyCreation:s})}},88658:function(e,n,t){"use strict";t.d(n,{L:function(){return i}});var a=t(49817);let i=e=>{let n;let{apiKeySource:t,accessToken:i,apiKey:o,inputMessage:r,chatHistory:s,selectedTags:l,selectedVectorStores:p,selectedGuardrails:m,selectedMCPTools:c,endpointType:u,selectedModel:g,selectedSdk:d}=e,_="session"===t?i:o,f=window.location.origin,h=r||"Your prompt here",b=h.replace(/\\/g,"\\\\").replace(/"/g,'\\"').replace(/\n/g,"\\n"),y=s.filter(e=>!e.isImage).map(e=>{let{role:n,content:t}=e;return{role:n,content:t}}),v={};l.length>0&&(v.tags=l),p.length>0&&(v.vector_stores=p),m.length>0&&(v.guardrails=m);let x=g||"your-model-name",w="azure"===d?'import openai\n\nclient = openai.AzureOpenAI(\n	api_key="'.concat(_||"YOUR_LITELLM_API_KEY",'",\n	azure_endpoint="').concat(f,'",\n	api_version="2024-02-01"\n)'):'import openai\n\nclient = openai.OpenAI(\n	api_key="'.concat(_||"YOUR_LITELLM_API_KEY",'",\n	base_url="').concat(f,'"\n)');switch(u){case a.KP.CHAT:{let e=Object.keys(v).length>0,t="";if(e){let e=JSON.stringify({metadata:v},null,2).split("\n").map(e=>" ".repeat(4)+e).join("\n").trim();t=",\n    extra_body=".concat(e)}let a=y.length>0?y:[{role:"user",content:h}];n='\nimport base64\n\n# Helper function to encode images to base64\ndef encode_image(image_path):\n    with open(image_path, "rb") as image_file:\n        return base64.b64encode(image_file.read()).decode(\'utf-8\')\n\n# Example with text only\nresponse = client.chat.completions.create(\n    model="'.concat(x,'",\n    messages=').concat(JSON.stringify(a,null,4)).concat(t,'\n)\n\nprint(response)\n\n# Example with image or PDF (uncomment and provide file path to use)\n# base64_file = encode_image("path/to/your/file.jpg")  # or .pdf\n# response_with_file = client.chat.completions.create(\n#     model="').concat(x,'",\n#     messages=[\n#         {\n#             "role": "user",\n#             "content": [\n#                 {\n#                     "type": "text",\n#                     "text": "').concat(b,'"\n#                 },\n#                 {\n#                     "type": "image_url",\n#                     "image_url": {\n#                         "url": f"data:image/jpeg;base64,{base64_file}"  # or data:application/pdf;base64,{base64_file}\n#                     }\n#                 }\n#             ]\n#         }\n#     ]').concat(t,"\n# )\n# print(response_with_file)\n");break}case a.KP.RESPONSES:{let e=Object.keys(v).length>0,t="";if(e){let e=JSON.stringify({metadata:v},null,2).split("\n").map(e=>" ".repeat(4)+e).join("\n").trim();t=",\n    extra_body=".concat(e)}let a=y.length>0?y:[{role:"user",content:h}];n='\nimport base64\n\n# Helper function to encode images to base64\ndef encode_image(image_path):\n    with open(image_path, "rb") as image_file:\n        return base64.b64encode(image_file.read()).decode(\'utf-8\')\n\n# Example with text only\nresponse = client.responses.create(\n    model="'.concat(x,'",\n    input=').concat(JSON.stringify(a,null,4)).concat(t,'\n)\n\nprint(response.output_text)\n\n# Example with image or PDF (uncomment and provide file path to use)\n# base64_file = encode_image("path/to/your/file.jpg")  # or .pdf\n# response_with_file = client.responses.create(\n#     model="').concat(x,'",\n#     input=[\n#         {\n#             "role": "user",\n#             "content": [\n#                 {"type": "input_text", "text": "').concat(b,'"},\n#                 {\n#                     "type": "input_image",\n#                     "image_url": f"data:image/jpeg;base64,{base64_file}",  # or data:application/pdf;base64,{base64_file}\n#                 },\n#             ],\n#         }\n#     ]').concat(t,"\n# )\n# print(response_with_file.output_text)\n");break}case a.KP.IMAGE:n="azure"===d?"\n# NOTE: The Azure SDK does not have a direct equivalent to the multi-modal 'responses.create' method shown for OpenAI.\n# This snippet uses 'client.images.generate' and will create a new image based on your prompt.\n# It does not use the uploaded image, as 'client.images.generate' does not support image inputs in this context.\nimport os\nimport requests\nimport json\nimport time\nfrom PIL import Image\n\nresult = client.images.generate(\n	model=\"".concat(x,'",\n	prompt="').concat(r,'",\n	n=1\n)\n\njson_response = json.loads(result.model_dump_json())\n\n# Set the directory for the stored image\nimage_dir = os.path.join(os.curdir, \'images\')\n\n# If the directory doesn\'t exist, create it\nif not os.path.isdir(image_dir):\n	os.mkdir(image_dir)\n\n# Initialize the image path\nimage_filename = f"generated_image_{int(time.time())}.png"\nimage_path = os.path.join(image_dir, image_filename)\n\ntry:\n	# Retrieve the generated image\n	if json_response.get("data") && len(json_response["data"]) > 0 && json_response["data"][0].get("url"):\n			image_url = json_response["data"][0]["url"]\n			generated_image = requests.get(image_url).content\n			with open(image_path, "wb") as image_file:\n					image_file.write(generated_image)\n\n			print(f"Image saved to {image_path}")\n			# Display the image\n			image = Image.open(image_path)\n			image.show()\n	else:\n			print("Could not find image URL in response.")\n			print("Full response:", json_response)\nexcept Exception as e:\n	print(f"An error occurred: {e}")\n	print("Full response:", json_response)\n'):"\nimport base64\nimport os\nimport time\nimport json\nfrom PIL import Image\nimport requests\n\n# Helper function to encode images to base64\ndef encode_image(image_path):\n	with open(image_path, \"rb\") as image_file:\n			return base64.b64encode(image_file.read()).decode('utf-8')\n\n# Helper function to create a file (simplified for this example)\ndef create_file(image_path):\n	# In a real implementation, this would upload the file to OpenAI\n	# For this example, we'll just return a placeholder ID\n	return f\"file_{os.path.basename(image_path).replace('.', '_')}\"\n\n# The prompt entered by the user\nprompt = \"".concat(b,'"\n\n# Encode images to base64\nbase64_image1 = encode_image("body-lotion.png")\nbase64_image2 = encode_image("soap.png")\n\n# Create file IDs\nfile_id1 = create_file("body-lotion.png")\nfile_id2 = create_file("incense-kit.png")\n\nresponse = client.responses.create(\n	model="').concat(x,'",\n	input=[\n			{\n					"role": "user",\n					"content": [\n							{"type": "input_text", "text": prompt},\n							{\n									"type": "input_image",\n									"image_url": f"data:image/jpeg;base64,{base64_image1}",\n							},\n							{\n									"type": "input_image",\n									"image_url": f"data:image/jpeg;base64,{base64_image2}",\n							},\n							{\n									"type": "input_image",\n									"file_id": file_id1,\n							},\n							{\n									"type": "input_image",\n									"file_id": file_id2,\n							}\n					],\n			}\n	],\n	tools=[{"type": "image_generation"}],\n)\n\n# Process the response\nimage_generation_calls = [\n	output\n	for output in response.output\n	if output.type == "image_generation_call"\n]\n\nimage_data = [output.result for output in image_generation_calls]\n\nif image_data:\n	image_base64 = image_data[0]\n	image_filename = f"edited_image_{int(time.time())}.png"\n	with open(image_filename, "wb") as f:\n			f.write(base64.b64decode(image_base64))\n	print(f"Image saved to {image_filename}")\nelse:\n	# If no image is generated, there might be a text response with an explanation\n	text_response = [output.text for output in response.output if hasattr(output, \'text\')]\n	if text_response:\n			print("No image generated. Model response:")\n			print("\\n".join(text_response))\n	else:\n			print("No image data found in response.")\n	print("Full response for debugging:")\n	print(response)\n');break;case a.KP.IMAGE_EDITS:n="azure"===d?'\nimport base64\nimport os\nimport time\nimport json\nfrom PIL import Image\nimport requests\n\n# Helper function to encode images to base64\ndef encode_image(image_path):\n	with open(image_path, "rb") as image_file:\n			return base64.b64encode(image_file.read()).decode(\'utf-8\')\n\n# The prompt entered by the user\nprompt = "'.concat(b,'"\n\n# Encode images to base64\nbase64_image1 = encode_image("body-lotion.png")\nbase64_image2 = encode_image("soap.png")\n\n# Create file IDs\nfile_id1 = create_file("body-lotion.png")\nfile_id2 = create_file("incense-kit.png")\n\nresponse = client.responses.create(\n	model="').concat(x,'",\n	input=[\n			{\n					"role": "user",\n					"content": [\n							{"type": "input_text", "text": prompt},\n							{\n									"type": "input_image",\n									"image_url": f"data:image/jpeg;base64,{base64_image1}",\n							},\n							{\n									"type": "input_image",\n									"image_url": f"data:image/jpeg;base64,{base64_image2}",\n							},\n							{\n									"type": "input_image",\n									"file_id": file_id1,\n							},\n							{\n									"type": "input_image",\n									"file_id": file_id2,\n							}\n					],\n			}\n	],\n	tools=[{"type": "image_generation"}],\n)\n\n# Process the response\nimage_generation_calls = [\n	output\n	for output in response.output\n	if output.type == "image_generation_call"\n]\n\nimage_data = [output.result for output in image_generation_calls]\n\nif image_data:\n	image_base64 = image_data[0]\n	image_filename = f"edited_image_{int(time.time())}.png"\n	with open(image_filename, "wb") as f:\n			f.write(base64.b64decode(image_base64))\n	print(f"Image saved to {image_filename}")\nelse:\n	# If no image is generated, there might be a text response with an explanation\n	text_response = [output.text for output in response.output if hasattr(output, \'text\')]\n	if text_response:\n			print("No image generated. Model response:")\n			print("\\n".join(text_response))\n	else:\n			print("No image data found in response.")\n	print("Full response for debugging:")\n	print(response)\n'):"\nimport base64\nimport os\nimport time\n\n# Helper function to encode images to base64\ndef encode_image(image_path):\n	with open(image_path, \"rb\") as image_file:\n			return base64.b64encode(image_file.read()).decode('utf-8')\n\n# Helper function to create a file (simplified for this example)\ndef create_file(image_path):\n	# In a real implementation, this would upload the file to OpenAI\n	# For this example, we'll just return a placeholder ID\n	return f\"file_{os.path.basename(image_path).replace('.', '_')}\"\n\n# The prompt entered by the user\nprompt = \"".concat(b,'"\n\n# Encode images to base64\nbase64_image1 = encode_image("body-lotion.png")\nbase64_image2 = encode_image("soap.png")\n\n# Create file IDs\nfile_id1 = create_file("body-lotion.png")\nfile_id2 = create_file("incense-kit.png")\n\nresponse = client.responses.create(\n	model="').concat(x,'",\n	input=[\n			{\n					"role": "user",\n					"content": [\n							{"type": "input_text", "text": prompt},\n							{\n									"type": "input_image",\n									"image_url": f"data:image/jpeg;base64,{base64_image1}",\n							},\n							{\n									"type": "input_image",\n									"image_url": f"data:image/jpeg;base64,{base64_image2}",\n							},\n							{\n									"type": "input_image",\n									"file_id": file_id1,\n							},\n							{\n									"type": "input_image",\n									"file_id": file_id2,\n							}\n					],\n			}\n	],\n	tools=[{"type": "image_generation"}],\n)\n\n# Process the response\nimage_generation_calls = [\n	output\n	for output in response.output\n	if output.type == "image_generation_call"\n]\n\nimage_data = [output.result for output in image_generation_calls]\n\nif image_data:\n	image_base64 = image_data[0]\n	image_filename = f"edited_image_{int(time.time())}.png"\n	with open(image_filename, "wb") as f:\n			f.write(base64.b64decode(image_base64))\n	print(f"Image saved to {image_filename}")\nelse:\n	# If no image is generated, there might be a text response with an explanation\n	text_response = [output.text for output in response.output if hasattr(output, \'text\')]\n	if text_response:\n			print("No image generated. Model response:")\n			print("\\n".join(text_response))\n	else:\n			print("No image data found in response.")\n	print("Full response for debugging:")\n	print(response)\n');break;default:n="\n# Code generation for this endpoint is not implemented yet."}return"".concat(w,"\n").concat(n)}},51601:function(e,n,t){"use strict";t.d(n,{p:function(){return i}});var a=t(19250);let i=async e=>{try{let n=await (0,a.modelHubCall)(e);if(console.log("model_info:",n),(null==n?void 0:n.data.length)>0){let e=n.data.map(e=>({model_group:e.model_group,mode:null==e?void 0:e.mode}));return e.sort((e,n)=>e.model_group.localeCompare(n.model_group)),e}return[]}catch(e){throw console.error("Error fetching model info:",e),e}}},49817:function(e,n,t){"use strict";var a,i,o,r;t.d(n,{KP:function(){return i},vf:function(){return l}}),(o=a||(a={})).IMAGE_GENERATION="image_generation",o.CHAT="chat",o.RESPONSES="responses",o.IMAGE_EDITS="image_edits",o.ANTHROPIC_MESSAGES="anthropic_messages",(r=i||(i={})).IMAGE="image",r.CHAT="chat",r.RESPONSES="responses",r.IMAGE_EDITS="image_edits",r.ANTHROPIC_MESSAGES="anthropic_messages";let s={image_generation:"image",chat:"chat",responses:"responses",image_edits:"image_edits",anthropic_messages:"anthropic_messages"},l=e=>{if(console.log("getEndpointType:",e),Object.values(a).includes(e)){let n=s[e];return console.log("endpointType:",n),n}return"chat"}},67479:function(e,n,t){"use strict";var a=t(57437),i=t(2265),o=t(52787),r=t(19250);n.Z=e=>{let{onChange:n,value:t,className:s,accessToken:l,disabled:p}=e,[m,c]=(0,i.useState)([]),[u,g]=(0,i.useState)(!1);return(0,i.useEffect)(()=>{(async()=>{if(l){g(!0);try{let e=await (0,r.getGuardrailsList)(l);console.log("Guardrails response:",e),e.guardrails&&(console.log("Guardrails data:",e.guardrails),c(e.guardrails))}catch(e){console.error("Error fetching guardrails:",e)}finally{g(!1)}}})()},[l]),(0,a.jsx)("div",{children:(0,a.jsx)(o.default,{mode:"multiple",disabled:p,placeholder:p?"Setting guardrails is a premium feature.":"Select guardrails",onChange:e=>{console.log("Selected guardrails:",e),n(e)},value:t,loading:u,className:s,options:m.map(e=>(console.log("Mapping guardrail:",e),{label:"".concat(e.guardrail_name),value:e.guardrail_name})),optionFilterProp:"label",showSearch:!0,style:{width:"100%"}})})}},97415:function(e,n,t){"use strict";var a=t(57437),i=t(2265),o=t(52787),r=t(19250);n.Z=e=>{let{onChange:n,value:t,className:s,accessToken:l,placeholder:p="Select vector stores",disabled:m=!1}=e,[c,u]=(0,i.useState)([]),[g,d]=(0,i.useState)(!1);return(0,i.useEffect)(()=>{(async()=>{if(l){d(!0);try{let e=await (0,r.vectorStoreListCall)(l);e.data&&u(e.data)}catch(e){console.error("Error fetching vector stores:",e)}finally{d(!1)}}})()},[l]),(0,a.jsx)("div",{children:(0,a.jsx)(o.default,{mode:"multiple",placeholder:p,onChange:n,value:t,loading:g,className:s,options:c.map(e=>({label:"".concat(e.vector_store_name||e.vector_store_id," (").concat(e.vector_store_id,")"),value:e.vector_store_id,title:e.vector_store_description||e.vector_store_id})),optionFilterProp:"label",showSearch:!0,style:{width:"100%"},disabled:m})})}}},function(e){e.O(0,[9820,1491,1526,2417,3709,9775,7908,9011,5319,7906,4851,6433,9888,8049,1052,2971,2117,1744],function(){return e(e.s=35831)}),_N_E=e.O()}]);