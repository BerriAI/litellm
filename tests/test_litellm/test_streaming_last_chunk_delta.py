"""Unit test to verify the last streaming chunk has delta with content='' not content=None

This test addresses the issue where LiteLLM was returning an empty delta object {} 
instead of delta with content='' for the last chunk with finish_reason='stop'.
"""
import pytest
from unittest.mock import Mock, MagicMock
from litellm.litellm_core_utils.streaming_handler import CustomStreamWrapper
from litellm.types.utils import ModelResponseStream, StreamingChoices, Delta
from litellm.litellm_core_utils.redact_messages import LiteLLMLoggingObject


def test_last_chunk_delta_has_empty_content_string():
    """Test that the last chunk with finish_reason has delta.content='' not delta.content=None"""
    
    # Create a mock logging object
    mock_logging_obj = Mock(spec=LiteLLMLoggingObject)
    mock_logging_obj.model_call_details = {
        "litellm_params": {}
    }
    mock_logging_obj._llm_caching_handler = None
    mock_logging_obj.completion_start_time = None
    mock_logging_obj.success_handler = Mock()
    mock_logging_obj.async_success_handler = Mock()
    mock_logging_obj._update_completion_start_time = Mock()
    
    # Create a CustomStreamWrapper instance
    stream_wrapper = CustomStreamWrapper(
        completion_stream=iter([]),  # Empty stream
        model="test-model",
        logging_obj=mock_logging_obj,
        custom_llm_provider="openai"
    )
    
    # Set up the wrapper to simulate receiving finish_reason
    stream_wrapper.received_finish_reason = "stop"
    stream_wrapper.sent_last_chunk = False
    stream_wrapper.sent_first_chunk = True
    
    # Create a model response
    model_response = stream_wrapper.model_response_creator()
    
    # Test the specific code path in return_processed_chunk_logic
    # where it handles the case when received_finish_reason is not None
    completion_obj = {"content": ""}
    response_obj = {}
    
    # The is_delta_empty check would return True for an empty delta
    _is_delta_empty = stream_wrapper.is_delta_empty(delta=model_response.choices[0].delta)
    assert _is_delta_empty is True
    
    # Now test the critical fix - when delta is empty and we have a finish reason
    # The delta should be created with content="" not content=None
    if _is_delta_empty:
        model_response.choices[0].delta = Delta(content="")
        model_response.choices[0].finish_reason = "stop"
    
    # Verify the delta has content attribute set to empty string
    assert hasattr(model_response.choices[0].delta, 'content')
    assert model_response.choices[0].delta.content == ""
    
    # Verify model_dump includes content key with empty string
    delta_dict = model_response.choices[0].delta.model_dump()
    assert 'content' in delta_dict
    assert delta_dict['content'] == ""
    
    # Verify it's not None
    assert model_response.choices[0].delta.content is not None
    assert delta_dict['content'] is not None


def test_openai_streaming_chunk_last_delta():
    """Test handling of OpenAI streaming chunks to ensure last chunk has proper delta"""
    
    # Create mock chunks simulating OpenAI streaming response
    class MockChoice:
        def __init__(self, delta_content=None, finish_reason=None):
            self.delta = Mock()
            self.delta.content = delta_content
            self.finish_reason = finish_reason
            self.index = 0
    
    class MockChunk:
        def __init__(self, choices):
            self.choices = choices
            self.id = "test-id"
            self.model = "gpt-3.5-turbo"
    
    # Simulate streaming chunks
    chunks = [
        MockChunk([MockChoice(delta_content="Hello", finish_reason=None)]),
        MockChunk([MockChoice(delta_content=" world", finish_reason=None)]),
        MockChunk([MockChoice(delta_content="", finish_reason="stop")]),  # Last chunk
    ]
    
    # Create a mock logging object
    mock_logging_obj = Mock(spec=LiteLLMLoggingObject)
    mock_logging_obj.model_call_details = {
        "litellm_params": {}
    }
    mock_logging_obj._llm_caching_handler = None
    mock_logging_obj.completion_start_time = None
    mock_logging_obj.success_handler = Mock()
    mock_logging_obj.async_success_handler = Mock()
    mock_logging_obj._update_completion_start_time = Mock()
    
    # Create stream wrapper
    stream_wrapper = CustomStreamWrapper(
        completion_stream=iter(chunks),
        model="gpt-3.5-turbo",
        logging_obj=mock_logging_obj,
        custom_llm_provider="openai"
    )
    
    # Process chunks
    processed_chunks = []
    for chunk in chunks:
        response_obj = stream_wrapper.handle_openai_chat_completion_chunk(chunk)
        if response_obj and response_obj.get("finish_reason") == "stop":
            # This simulates the last chunk processing
            stream_wrapper.received_finish_reason = "stop"
            
            # Create the final model response
            model_response = stream_wrapper.model_response_creator()
            model_response.choices[0].delta = Delta(content="")
            model_response.choices[0].finish_reason = "stop"
            
            # Verify the delta is correct
            assert model_response.choices[0].delta.content == ""
            assert hasattr(model_response.choices[0].delta, 'content')
            
            delta_dict = model_response.choices[0].delta.model_dump()
            assert delta_dict['content'] == ""


if __name__ == "__main__":
    test_last_chunk_delta_has_empty_content_string()
    test_openai_streaming_chunk_last_delta()
    print("âœ… All tests passed!")