#!/usr/bin/env python3
"""
Debug streaming issues with chat provider
"""

def test_streaming_response_transformation():
    """Test the streaming response transformation with various event types"""
    print("ğŸ§ª Testing streaming response transformation...")
    
    try:
        from litellm.llms.chat.transformation import ChatConfig
        
        config = ChatConfig()
        
        # Test various Responses API event types
        test_events = [
            # Response created event
            {
                "type": "response.created",
                "response": {
                    "id": "resp_123",
                    "created_at": 1234567890,
                    "model": "gpt-4o"
                }
            },
            
            # Output item added
            {
                "type": "response.output_item.added",
                "response_id": "resp_123",
                "output_item": {
                    "type": "message",
                    "role": "assistant"
                }
            },
            
            # Content part added
            {
                "type": "response.content_part.added",
                "response_id": "resp_123",
                "part": {
                    "type": "text",
                    "text": "Hello"
                }
            },
            
            # Content part done
            {
                "type": "response.content_part.done",
                "response_id": "resp_123"
            },
            
            # Response done
            {
                "type": "response.done",
                "response": {
                    "id": "resp_123",
                    "created_at": 1234567890,
                    "model": "gpt-4o",
                    "usage": {
                        "input_tokens": 10,
                        "output_tokens": 5,
                        "total_tokens": 15
                    }
                }
            },
            
            # Unknown event type
            {
                "type": "response.unknown_event",
                "response_id": "resp_123",
                "data": "test"
            },
            
            # Empty chunk
            None,
            
            # Invalid chunk
            "invalid"
        ]
        
        for i, event in enumerate(test_events):
            print(f"\n  Test {i+1}: {event.get('type') if isinstance(event, dict) else type(event)}")
            
            try:
                result = config.transform_streaming_response("gpt-4o", event, None)
                
                if result is None:
                    print(f"    âœ… Returned None (acceptable for some events)")
                elif isinstance(result, dict) and "object" in result:
                    print(f"    âœ… Valid chunk: {result.get('object')}")
                    if "choices" in result:
                        delta = result["choices"][0].get("delta", {})
                        if "content" in delta:
                            print(f"    ğŸ“ Content: '{delta['content']}'")
                        if "role" in delta:
                            print(f"    ğŸ‘¤ Role: {delta['role']}")
                else:
                    print(f"    âš ï¸  Unexpected result: {type(result)}")
                    
            except Exception as e:
                print(f"    âŒ Exception: {e}")
                import traceback
                traceback.print_exc()
        
        return True
        
    except Exception as e:
        print(f"âŒ Streaming test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_stream_request_setup():
    """Test how streaming requests are set up"""
    print("\nğŸ§ª Testing stream request setup...")
    
    try:
        from litellm.llms.chat.transformation import ChatConfig
        
        config = ChatConfig()
        
        # Test streaming request transformation
        messages = [{"role": "user", "content": "Hello"}]
        optional_params = {"stream": True, "max_tokens": 100}
        litellm_params = {"custom_llm_provider": "chat"}
        headers = {}
        
        print("  Input parameters:")
        print(f"    messages: {messages}")
        print(f"    optional_params: {optional_params}")
        print(f"    litellm_params: {litellm_params}")
        
        result = config.transform_request(
            model="gpt-4o",
            messages=messages,
            optional_params=optional_params,
            litellm_params=litellm_params,
            headers=headers
        )
        
        print(f"\n  âœ… Request transformation successful!")
        print(f"    Stream parameter: {result.get('stream')}")
        print(f"    Model: {result.get('model')}")
        print(f"    Input items: {len(result.get('input', []))}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Stream request test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run streaming debug tests"""
    print("ğŸ”§ Chat Provider Streaming Debug Tests")
    print("=" * 50)
    
    tests = [
        ("Streaming Response Transformation", test_streaming_response_transformation),
        ("Stream Request Setup", test_stream_request_setup),
    ]
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        test_func()
    
    print("\n" + "="*50)
    print("ğŸ¯ Debugging Notes:")
    print("  â€¢ Check for 'NoneType' object is not an iterator errors")
    print("  â€¢ Verify streaming events are properly formatted")
    print("  â€¢ Ensure transform_streaming_response handles all cases")
    print("  â€¢ Look for missing stream parameter in requests")

if __name__ == "__main__":
    main()