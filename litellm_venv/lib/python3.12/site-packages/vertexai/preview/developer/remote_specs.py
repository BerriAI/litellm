# -*- coding: utf-8 -*-

# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

"""Remote workload specs and helper functions for developer.mark.fit.

"""

import dataclasses
import json
import os
import tempfile

from typing import Any, Dict, List, Optional

from google.cloud.aiplatform import base
from google.cloud.aiplatform.utils import gcs_utils
from google.cloud.aiplatform.utils import worker_spec_utils
from google.cloud.aiplatform.preview import resource_pool_utils
from vertexai.preview._workflow.serialization_engine import (
    serializers,
)

try:
    import torch
except ImportError:
    pass

_LOGGER = base.Logger(__name__)

_LITERAL: str = "literal"
_PARQUET: str = "parquet"
_CLOUDPICKLE: str = "cloudpickle"

# Constants for serializer.
_SERIALIZER = frozenset([_LITERAL, _PARQUET, _CLOUDPICKLE])

_METADATA_FILE_NAME = "metadata"
_DATA_FILE_NAME = "data.parquet"
_FLOAT16 = "float16"
_FLOAT32 = "float32"
_FLOAT64 = "float64"
_INT8 = "int8"
_INT16 = "int16"
_INT32 = "int32"
_INT64 = "int64"
_UINT8 = "uint8"
_UINT16 = "uint16"
_UINT32 = "uint32"
_UINT64 = "uint64"
_SUPPORTED_NUMERICAL_DTYPES = [
    _FLOAT16,
    _FLOAT32,
    _FLOAT64,
    _INT8,
    _INT16,
    _INT32,
    _INT64,
    _UINT8,
    _UINT16,
    _UINT32,
    _UINT64,
]
_DENSE = "dense"

# Constants for deserializer.
_DESERIALIZER = frozenset([_LITERAL, _CLOUDPICKLE])

# Constants for Cluster and ClusterSpec
_CHIEF = "workerpool0"
_WORKER = "workerpool1"
_SERVER = "workerpool2"
_EVALUATOR = "workerpool3"
_WORKER_POOLS = frozenset([_CHIEF, _WORKER, _SERVER, _EVALUATOR])
_CLUSTER = "cluster"
_TASK = "task"
_TYPE = "type"
_INDEX = "index"
_TRIAL = "trial"

_CLUSTER_SPEC = "CLUSTER_SPEC"
_MASTER_ADDR = "MASTER_ADDR"
_MASTER_PORT = "MASTER_PORT"


def _gen_gcs_path(base_dir: str, name: str) -> str:
    """Generates a GCS path for a file or a directory.

    The created path can be used for either a file or a directory. If it is a
    file, we can directly write to it. If it is a directory, file paths could
    be generated by joining the derectly path and the file names.

    Example usages:
    1. When passing in parameters to a custom job, we will be able to write
    serialized parameter value content to a GCS path in 'cloudpickle' mode.
    2. We will also provide GCS paths so that the custom job can write the
    output parameter values to the dedicated paths.

    Args:
        base_dir (str):
            Required. The base GCS directory. Must be a valid GCS path that
            starts with 'gs://'.
        name (str):
            Required. The name of a file or directory. If name ends with '/',
            removes '/' for consistency since we do not need the suffix to
            identify the path as a directory.
    Returns:
        The generated GCS path.
    Raises:
        ValueError if the input base_dir is not a valid GCS path.
    """
    if not base_dir.startswith("gs://"):
        raise ValueError(f"base_dir {base_dir} is not a valid GCS path.")
    name = name[:-1] if name.endswith("/") else name
    return os.path.join(base_dir, name)


def _get_argument_name(name: str) -> str:
    """Gets an argument name for the inputs and outputs of a container.

    1. If the name contains dots such as a.b.arg_name or self.arg_name, use the
    string following the right-most dot (arg_name) as the argument name.
    2. If the name has a single leading underscore, such as _arg_name, remove
    the leading underscore in the argument name (arg_name). If the name has a
    double leading underscore such as __arg_name, use the argument name
    __arg_name directly.

    Args:
        name (str):
            Required. The name of the parameter in the InputParameterSpec.

    Returns:
        The name of the argument in the container.
    """
    argument_name = name.split(".")[-1]
    if argument_name.startswith("_") and not argument_name.startswith("__"):
        argument_name = argument_name[1:]
    if not argument_name:
        raise ValueError(f"Failed to get argument name from name {name}.")
    return argument_name


@dataclasses.dataclass
class _FeatureMetadata:
    dtype: str
    feature_type: str = _DENSE


@dataclasses.dataclass
class _CategoricalFeatureMetadata:
    dtype: str
    categories: List[Any]
    feature_type: str = _DENSE


@dataclasses.dataclass
class _TaskInfo:
    """Describes the task of the particular node on which code is running.

    Args:
        task_type (str):
            Required. The type of worker pool this task is running in. One of 'workerpool0' for chief, 'workerpool1' for worker, 'workerpool2' for server or 'workerpool3' for evaluator.
        task_index (int):
            Required. The zero-based index of the task. If a training job has two workers, this value is set to 0 on one and 1 on the other.
        task_trial (int):
            Optional. The identifier of the hyperparameter tuning trial currently running.
    """

    task_type: str
    task_index: int
    task_trial: int = None


class _InputParameterSpec:
    """Input parameter spec for remote trainers."""

    def __init__(
        self,
        name: str,
        argument_name: Optional[str] = None,
        serializer: str = _LITERAL,
    ) -> None:
        """Initializes an _InputParameterSpec instance.

        When creating CustomJob spec, each _InputParameterSpec will be
        transformed into a custom job input.

        Args:
            name (str):
                Required. The parameter name that stores the input value.
            argument_name (str):
                Optional. The argument name for the custom job input. If not
                specified, an argument_name will be derived from name.
            serializer (str):
                Optional. The serializer for the input. Must be one of
                'literal', 'parquet', and 'cloudpickle'.

        Raises:
            ValueError: If name or serializer is invalid.
        """
        if not name:
            raise ValueError("Input parameter name cannot be empty.")
        self.name = name
        self.argument_name = argument_name or _get_argument_name(name)
        if serializer not in _SERIALIZER:
            raise ValueError(
                f"Invalid serializer {serializer} for {name}. Please"
                f"choose one of {list(_SERIALIZER)}."
            )
        self.serializer = serializer

    def format_arg(self, input_dir: str, binding: Dict[str, Any]) -> Any:
        """Formats an argument based on the spec.

        Args:
            input_dir (str):
                Required. The GCS input directory to save the serialized input
                value when necessary.
            binding (Dict[str, Any]):
                Required. A dictionary that contains maps an input name to its
                value.

        Returns:
            The formatted argument.

        Raises:
            ValueError if the input is not found in binding, tries to serialize
            a non-pandas.DataFrame to parquet, or the serialization format is
            not supported.
        """
        try:
            # pylint: disable=g-import-not-at-top
            import pandas as pd
        except ImportError:
            raise ImportError(
                "pandas is not installed and is required for remote training."
            ) from None
        if self.name not in binding:
            raise ValueError(f"Input {self.name} not found in binding: " f"{binding}.")

        value = binding[self.name]
        if self.serializer == _LITERAL:
            return value

        gcs_path = _gen_gcs_path(input_dir, self.argument_name)
        if self.serializer == _PARQUET:
            if not isinstance(value, pd.DataFrame):
                raise ValueError(
                    "Parquet serializer is only supported for "
                    f"pandas.DataFrame, but {self.name} has type "
                    f"{type(value)}."
                )
            # Serializes data
            data_serializer = serializers.PandasDataSerializer()
            data_path = _gen_gcs_path(gcs_path, _DATA_FILE_NAME)
            data_serializer.serialize(
                to_serialize=value,
                gcs_path=data_path,
            )

            # Serializes feature metadata
            metadata_serializer = serializers.CloudPickleSerializer()
            metadata_path = _gen_gcs_path(gcs_path, _METADATA_FILE_NAME)
            feature_metadata = _generate_feature_metadata(value)
            metadata_serializer.serialize(
                to_serialize=feature_metadata, gcs_path=metadata_path
            )

        elif self.serializer == _CLOUDPICKLE:
            serializer = serializers.CloudPickleSerializer()
            serializer.serialize(
                to_serialize=value,
                gcs_path=gcs_path,
            )

        else:
            raise ValueError(
                f"Unsupported serializer: {self.serializer}."
                "The input serializer must be one of "
                f"{_SERIALIZER}."
            )
        return gcs_path


class _OutputParameterSpec:
    """Output parameter spec for remote trainers."""

    def __init__(
        self,
        name: str,
        argument_name: Optional[str] = None,
        deserializer: Optional[str] = _LITERAL,
    ) -> None:
        """Initializes an OutputParameterSpec instance.

        When creating CustomJob spec, each OutputParameterSpec will be
        transformed into a custom job argument that will store the output value.

        Args:
            name (str):
                Required. The parameter name that will store the output value.
            argument_name (str):
                Optional. The argument name for the custom job argument. If not
                specified, an argument_name will be derived from name.
            deserializer (str):
                Optional. The deserializer for the output. Must be one of
                'literal', and 'cloudpickle'.

        Raises:
            ValueError: If name or deserializer is invalid.
        """
        if not name:
            raise ValueError("Output parameter name cannot be empty.")
        self.name = name
        self.argument_name = argument_name or _get_argument_name(name)
        if deserializer not in _DESERIALIZER:
            raise ValueError(
                f"Invalid deserializer {deserializer} for {name}. Please"
                f"choose one of {list(_DESERIALIZER)}."
            )
        self.deserializer = deserializer

    def deserialize_output(self, gcs_path: str) -> Any:
        """Deserializes an output based on the spec.

        Args:
            gcs_path (str):
                Required. The gcs path containing the output.

        Returns:
            The deserialized output.

        Raises:
            ValueError if the deserialization format is unsupported.
        """
        if self.deserializer == _LITERAL:
            with tempfile.NamedTemporaryFile() as temp_file:
                gcs_utils.download_file_from_gcs(gcs_path, temp_file.name)
                with open(temp_file.name, "r") as f:
                    return f.read()
        elif self.deserializer == _CLOUDPICKLE:
            serializer = serializers.CloudPickleSerializer()
            return serializer.deserialize(serialized_gcs_path=gcs_path)
        else:
            raise ValueError(f"Unsupported deserializer: {self.deserializer}.")


def _generate_feature_metadata(df: Any) -> Dict[str, Any]:
    """Helper function to generate feature metadata from a pandas DataFrame.

    When column types are not supported, the corresponding columns are excluded
    from feature metadata.

    Args:
        df (pandas.DataFrame):
            Required. A DataFrame to generate feature metadata from.

    Returns:
        A dictionary that maps column names to metadata.

    Raises:
        ValueError if df is not a valid/ supported DataFrame.
    """
    try:
        # pylint: disable=g-import-not-at-top
        import pandas as pd
    except ImportError:
        raise ImportError(
            "pandas is not installed and is required for remote training."
        ) from None

    if not isinstance(df, pd.DataFrame):
        raise ValueError(
            "Generating feature metadata is only supported for "
            f"pandas.DataFrame, but {df} has type {type(df)}."
        )

    feature_metadata = {}
    for col in df.columns:
        if df[col].dtypes in _SUPPORTED_NUMERICAL_DTYPES:
            feature_metadata[str(col)] = dataclasses.asdict(
                _FeatureMetadata(str(df[col].dtypes))
            )
        # Ignores categorical columns that are not integers.
        elif df[col].dtypes == "category" and df[col].cat.categories.dtype == _INT64:
            categories = df[col].cat.categories.tolist()
            feature_metadata[str(col)] = dataclasses.asdict(
                _CategoricalFeatureMetadata(_INT64, categories)
            )
        else:
            # Ignores unsupported column type.
            pass
    return feature_metadata


class _Cluster:
    """Represents a Cluster as a set of "tasks".

    Task type or worker pool can be one of chief, worker, server or evaluator.

    To create a cluster with two task types and three tasks, specify the
    mapping from worker pool to list of network addresses.

    ```python
    cluster = Cluster({"workerpool0": ["cmle-training-workerpool0-ab-0:2222"],
                       "workerpool1": ["cmle-training-workerpool1-ab-0:2222",
                                       "cmle-training-workerpool1-ab-1:2222"]})
    ```
    """

    def __init__(self, cluster_info: Dict[str, Any]):
        """Initializes a Cluster instance.

        The cluster description contains a list of tasks for each
        task type or worker pool specified in a CustomJob.

        Args:
            cluster_info (Dict[str, Any]): Required. The cluster description
                containing the list of tasks for each task type.

        Raises:
           ValueError: If cluster description contains invalid task types.
        """
        for task_type in cluster_info:
            if task_type not in _WORKER_POOLS:
                raise ValueError(
                    f"Invalid task type: {task_type}. Must be one of {_WORKER_POOLS}."
                )
        self.cluster_info = cluster_info

    # Different worker pool types
    @property
    def chief_task_type(self) -> str:
        return _CHIEF

    @property
    def worker_task_type(self) -> str:
        return _WORKER

    @property
    def server_task_type(self) -> str:
        return _SERVER

    @property
    def evaluator_task_type(self) -> str:
        return _EVALUATOR

    @property
    def task_types(self) -> List[str]:
        """Returns a list of task types in this cluster.

        Returns:
            A list of task types in this cluster.
        """
        return list(self.cluster_info.keys())

    def get_num_tasks(self, task_type):
        """Returns the number of tasks of a given task type.

        Args:
            task_type (str): The task type.

        Returns:
            The number of tasks of the given task type.
        """
        if task_type not in self.cluster_info:
            return 0
        return len(self.cluster_info[task_type])

    def get_task_addresses(self, task_type):
        """Returns list of task address for the task type.

        Args:
            task_type (str): The task type.

        Returns:
            A list of task address for the given task type.

        Raises:
            ValueError: If the task type passed does not exist in the cluster.
        """
        if task_type not in self.cluster_info:
            raise ValueError(f"No such task type in cluster: {task_type}")
        return self.cluster_info[task_type]


class _ClusterSpec:
    """ClusterSpec for a distributed training job."""

    def __init__(self, cluster_spec: Dict[str, Any]):
        """Initializes a ClusterSpec instance.

        Vertex AI populates an environment variable, CLUSTER_SPEC, on every
        replica to describe how the overall cluster is set up. For
        distributed
        training, this environment variable will be used to create a
        ClusterSpec.

        A sample CLUSTER_SPEC:
        ```
        {
            "cluster": {
                "workerpool0": [
                    "cmle-training-workerpool0-ab-0:2222"
                ],
                "workerpool1": [
                    "cmle-training-workerpool1-ab-0:2222",
                    "cmle-training-workerpool1-ab-1:2222"
                ],
                "workerpool2": [
                    "cmle-training-workerpool2-ab-0:2222"
                ],
                "workerpool3": [
                    "cmle-training-workerpool3-ab-0:2222"
                ]
            },
            "environment":"cloud",
            "task":{
                "type": "workerpool0",
                "index": 0
            }
        }
        ```
        Args:
            cluster_spec (Dict[str, Any]): Required. The cluster spec
              containing the cluster and current task specification.

        Raises:
            ValueError: If `cluster_spec` is missing required keys.
        """
        if _CLUSTER not in cluster_spec or _TASK not in cluster_spec:
            raise ValueError(f"`cluster_spec` must contain {_CLUSTER} and {_TASK}")
        self.cluster = _Cluster(cluster_spec[_CLUSTER])
        self.task = _TaskInfo(
            task_type=cluster_spec[_TASK][_TYPE],
            task_index=cluster_spec[_TASK][_INDEX],
            task_trial=cluster_spec[_TASK].get(_TRIAL, None),
        )

    def get_rank(self):
        """Returns the world rank of the current task.

        Returns:
            The world rank of the current task.
        """
        task_type = self.task.task_type
        task_index = self.task.task_index

        if task_type == self.cluster.chief_task_type:
            return 0
        if task_type == self.cluster.worker_task_type:
            return task_index + 1

        num_workers = self.cluster.get_num_tasks(self.cluster.worker_task_type)
        if task_type == self.cluster.server_task_type:
            return num_workers + task_index + 1

        num_ps = self.cluster.get_num_tasks(self.cluster.server_task_type)
        if task_type == self.cluster.evaluator_task_type:
            return num_ps + num_workers + task_index + 1

    def get_world_size(self):
        """Returns the world size (total number of workers) for the current run.

        Returns:
            The world size for the current run.
        """
        num_chief = self.cluster.get_num_tasks(self.cluster.chief_task_type)
        num_workers = self.cluster.get_num_tasks(self.cluster.worker_task_type)
        num_ps = self.cluster.get_num_tasks(self.cluster.server_task_type)
        num_evaluators = self.cluster.get_num_tasks(self.cluster.evaluator_task_type)

        return num_chief + num_workers + num_ps + num_evaluators

    def get_chief_address_port(self):
        """Returns address and port for chief task.

        Returns:
            A tuple of task address and port.

        Raises:
            ValueError: If the chief task type does not exist in the cluster
        """
        if self.cluster.chief_task_type not in self.cluster.task_types:
            raise ValueError("Cluster must have a chief task.")
        chief_task = self.cluster.get_task_addresses(self.cluster.chief_task_type)[0]
        address, port = chief_task.split(":")
        return address, int(port)


# pylint: disable=protected-access
class WorkerPoolSpec(worker_spec_utils._WorkerPoolSpec):
    """Wraps class that holds a worker pool spec configuration.

    Attributes:
        replica_count (int):
            The number of worker replicas.
        machine_type (str):
            The type of machine to use for remote training.
        accelerator_count (int):
            The number of accelerators to attach to a worker replica.
        accelerator_type (str):
            Hardware accelerator type. One of ACCELERATOR_TYPE_UNSPECIFIED,
            NVIDIA_TESLA_A100, NVIDIA_TESLA_P100, NVIDIA_TESLA_V100,
            NVIDIA_TESLA_K80, NVIDIA_TESLA_T4, NVIDIA_TESLA_P4
        boot_disk_type (str):
            Type of the boot disk (default is `pd-ssd`).
            Valid values: `pd-ssd` (Persistent Disk Solid State Drive) or
            `pd-standard` (Persistent Disk Hard Disk Drive).
        boot_disk_size_gb (int):
            Size in GB of the boot disk (default is 100GB).
            boot disk size must be within the range of [100, 64000].
    """


@dataclasses.dataclass
class WorkerPoolSpecs:
    """A class that holds the worker pool specs configuration for a remote job.

    Attributes:
        chief (WorkerPoolSpec):
            The `cheif` or `workerpool0` worker pool spec configuration.
        worker (WorkerPoolSpec):
            The `worker` or `workerpool1` worker pool spec configuration.
        server (WorkerPoolSpec):
            The `server` or `workerpool2` worker pool spec configuration.
        evaluator (WorkerPoolSpec):
            The `evaluator` or `workerpool3` worker pool spec configuration.
    """

    chief: WorkerPoolSpec
    worker: Optional[WorkerPoolSpec] = None
    server: Optional[WorkerPoolSpec] = None
    evaluator: Optional[WorkerPoolSpec] = None


def _prepare_worker_pool_specs(
    worker_pool_specs: WorkerPoolSpecs,
    image_uri: str,
    command: Optional[List[Any]] = [],
    args: Optional[List[Any]] = [],
):
    """Return each worker pools spec in order for Vertex AI Training as a list of dicts.

    Args:
        worker_pool_specs (WorkerPoolSpecs): Required. Worker pool specs configuration for a remote job.
        image_uri (str): Required. Image uri for training.
        command (str): Command for training.
        args (str): Args for training.

    Returns:
        Ordered list of worker pool specs for Vertex AI Training.

    Raises:
        ValueError: If replica_count for cheif worker pool spec is greater than 1.
    """

    if worker_pool_specs.chief.replica_count > 1:
        raise ValueError(
            "Chief worker pool spec replica_count cannot be greater than 1."
        )
    spec_order = [
        worker_pool_specs.chief,
        worker_pool_specs.worker,
        worker_pool_specs.server,
        worker_pool_specs.evaluator,
    ]
    formatted_specs = [{} if not spec else spec.spec_dict for spec in spec_order]

    # Remove empty trailing worker pool specs
    for i in reversed(range(len(spec_order))):
        if spec_order[i]:
            break
        formatted_specs.pop()

    # Add container spec to each non-empty worker pool spec
    for spec in formatted_specs:
        if spec:
            spec["container_spec"] = {
                "image_uri": image_uri,
                "command": command,
                "args": args,
            }

    return formatted_specs


def _verify_specified_remote_config_values(
    worker_pool_specs: WorkerPoolSpecs,
    machine_type: str,
    accelerator_type: str,
    accelerator_count: int,
    replica_count: Optional[int] = None,
    boot_disk_type: Optional[str] = None,
    boot_disk_size_gb: Optional[int] = None,
):
    """Helper to validate if remote_config.worker_pool_specs is set, other remote job config values are not."""
    if worker_pool_specs and (
        machine_type
        or accelerator_type
        or accelerator_count
        or replica_count
        or boot_disk_type
        or boot_disk_size_gb
    ):
        raise ValueError(
            "Cannot specify both 'worker_pool_specs' and ['machine_type', 'accelerator_type', 'accelerator_count', 'replica_count', 'boot_disk_type', 'boot_disk_size_gb']."
        )


def _get_cluster_spec() -> _ClusterSpec:
    """Helper to check for CLUSTER_SPEC environment variable and return object if it exists."""
    cluster_spec_str = os.getenv(_CLUSTER_SPEC, "")
    if cluster_spec_str:
        return _ClusterSpec(json.loads(cluster_spec_str))
    return None


def _get_output_path_for_distributed_training(base_dir, name) -> str:
    """Helper to get output path for distributed training."""
    cluster_spec = _get_cluster_spec()
    if cluster_spec:
        task_type = cluster_spec.task.task_type
        task_id = cluster_spec.task.task_index

        if task_type != cluster_spec.cluster.chief_task_type:
            temp_path = os.path.join(base_dir, "temp")
            os.makedirs(temp_path, exist_ok=True)
            temp_path = os.path.join(temp_path, f"{task_type}_{task_id}")
            return temp_path

    return os.path.join(base_dir, name)


def _get_keras_distributed_strategy(enable_distributed: bool, accelerator_count: int):
    """Returns distribute strategy for Keras distributed training.

    For multi-worker training, use tf.distribute.MultiWorkerMirroredStrategy().
    For single worker, multi-GPU training, use tf.distribute.MirroredStrategy().
    For non-distributed training, return None. Requires TensorFlow >= 2.12.0.

    Args:
        enable_distributed (boolean): Required. Whether distributed training is enabled.
        accelerator_count (int): Accelerator count specified for single worker training.

    Returns:
       A tf.distribute.Strategy.
    """
    import tensorflow as tf

    if tf.__version__ < "2.13.0":
        raise ValueError("TensorFlow version < 2.13.0 is not supported.")

    if enable_distributed:
        cluster_spec = _get_cluster_spec()
        # Multiple workers, use tf.distribute.MultiWorkerMirroredStrategy().
        if cluster_spec and len(cluster_spec.cluster.task_types) >= 2:
            return tf.distribute.MultiWorkerMirroredStrategy()
        # Single worker, use tf.distribute.MirroredStrategy(). We validate accelerator_count > 1 before
        # creating CustomJob.
        else:
            return tf.distribute.MirroredStrategy()
    # Multi-GPU training, but enable_distributed is false, use tf.distribute.MirroredStrategy().
    elif accelerator_count and accelerator_count > 1:
        return tf.distribute.MirroredStrategy()
    # Not distributed, return None.
    else:
        return None


def _set_keras_distributed_strategy(model: Any, strategy: Any):
    """Returns a model compiled within the scope of the specified distribute strategy.

    Requires TensorFlow >= 2.12.0.

    Args:
        model (Any): Required. An instance of a Keras model.
        strategy (tf.distribute.Strategy): The distribute strategy.

    Returns:
        A tf.distribute.Strategy.
    """
    # Clone and compile model within scope of chosen strategy.
    import tensorflow as tf

    if tf.__version__ < "2.13.0":
        raise ValueError("TensorFlow version < 2.13.0 is not supported.")

    with strategy.scope():
        cloned_model = tf.keras.models.clone_model(model)
        cloned_model.compile_from_config(model.get_compile_config())

    return cloned_model


def setup_pytorch_distributed_training(model: Any) -> Any:
    """Sets up environment for PyTorch distributed training.

    The number of nodes or processes (`world_size`) is the number of
    workers being used for the training run. This helper can be called
    within the Vertex remote training-enabled function of a custom model
    built on top of `torch.nn.Module`.

    Example Usage:
        ```
        vertexai.init(
            project="my-project",
            location="my-location",
            staging_bucket="gs://my-bucket",
        )
        vertexai.preview.init(remote=True)

        class MyModel(vertexai.preview.VertexModel, torch.nn.Module):
            ...

            @vertexai.preview.developer.mark.train()
            def my_train_method(self, ...):
                self = setup_pytorch_distributed_training(self)
                ...

        model = MyModel(...)

        # This will execute distributed, remote training
        model.my_train_method(...)
        ```
    Args:
        model (Any): Required. An instance of a custom PyTorch model.

    Returns:
        A custom model built on top of `torch.nn.Module` wrapped in DistributedDataParallel.
    """
    if not model.cluster_spec:  # cluster_spec is populated for multi-worker training
        return model

    device = "cuda" if model._enable_cuda else "cpu"
    rank = model.cluster_spec.get_rank()
    world_size = model.cluster_spec.get_world_size()
    address, port = model.cluster_spec.get_chief_address_port()

    os.environ[_MASTER_ADDR] = address
    os.environ[_MASTER_PORT] = str(port)

    torch.distributed.init_process_group(
        backend="nccl" if device == "cuda" else "gloo",
        rank=rank,
        world_size=world_size,
    )

    if device == "cuda":
        model.to(device)
    model = torch.nn.parallel.DistributedDataParallel(model)

    _LOGGER.info(
        f"Initialized process rank: {rank}, world_size: {world_size}, device: {device}",
    )
    return model


# pylint: disable=protected-access
class ResourcePool(resource_pool_utils._ResourcePool):
    """Wraps class that holds a worker pool spec configuration.

    Attributes:
        replica_count (int):
            The number of worker replicas.
        machine_type (str):
            The type of machine to use for remote training.
        accelerator_count (int):
            The number of accelerators to attach to a worker replica.
        accelerator_type (str):
            Hardware accelerator type. One of ACCELERATOR_TYPE_UNSPECIFIED,
            NVIDIA_TESLA_A100, NVIDIA_TESLA_P100, NVIDIA_TESLA_V100,
            NVIDIA_TESLA_K80, NVIDIA_TESLA_T4, NVIDIA_TESLA_P4
        boot_disk_type (str):
            Type of the boot disk (default is `pd-ssd`).
            Valid values: `pd-ssd` (Persistent Disk Solid State Drive) or
            `pd-standard` (Persistent Disk Hard Disk Drive).
        boot_disk_size_gb (int):
            Size in GB of the boot disk (default is 100GB).
            boot disk size must be within the range of [100, 64000].
    """
