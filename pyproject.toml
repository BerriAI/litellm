[tool.poetry]
name = "litellm"
version = "1.16.19"
description = "Library to easily interface with LLM API providers"
authors = ["BerriAI"]
license = "MIT License"
readme = "README.md"

[tool.poetry.dependencies]
python = ">3.9.7, <3.12"

python-dotenv ="^0.12.0"                                    # for env 
tiktoken = "^0.4.0"                                         # for calculating usage
importlib-metadata = "^6.8.0"                               # for random utils
tokenizers = "^0.14.0"                                      # for calculating usage
click = "^8.1.7"                                            # for proxy cli 
jinja2 = "^3.1.2"                                           # for prompt templates
certifi = "^2023.7.22"                                      # [TODO] clean up 
aiohttp = "^3.9.0"                                          # for network calls

anyio = {version = "^4.2.0", optional = true}               # openai + http req.
openai = {version = "^1.0.0", optional = true}              # openai req. 
fastapi = {version = "^0.100.0", optional = true}           # server dep
pydantic = {version = "^2.5", optional = true}              # openai req. 
backoff = {version = "^2.2.1", optional = true}             # server dep
pyyaml = {version = "^6.0", optional = true}                # server dep
uvicorn = {version = "^0.22.0", optional = true}            # server dep
boto3 = {version = "^1.28.58", optional = true}             # aws bedrock/sagemaker calls
redis = {version = "^4.6.0", optional = true}               # caching
prisma = {version = "^0.11.0", optional = true}             # for db
mangum = {version = "^0.17.0", optional = true}             # for aws lambda functions
google-generativeai = {version = "^0.1.0", optional = true} # for vertex ai calls
async_generator = {version = "^1.10.0", optional = true}    # for async ollama calls
traceloop-sdk = {version = "^0.5.3", optional = true}       # for open telemetry logging
langfuse = {version = "^2.0.0", optional = true}            # for langfuse self-hosted logging
orjson = {version = "^3.9.7", optional = true}              # fast /embedding responses

[tool.poetry.extras]
proxy = [
    "anyio",
    "openai",
    "fastapi",
    "pydantic",
    "backoff",
    "pyyaml",
    "uvicorn",
]

extra_proxy = [
    "boto3",
    "redis",
    "prisma",
    "mangum",
    "google-generativeai",
    "async_generator",
    "traceloop-sdk",
    "langfuse",
    "orjson",
]


[tool.poetry.scripts]
litellm = 'litellm:run_server'

[tool.poetry.group.dev.dependencies]
flake8 = "^6.1.0"
black = "^23.12.0"
pytest = "^7.4.3"
mypy = "^1.8.0"
pytest-async = "^0.1.1"
numpydoc = "^1.6.0"
setuptools = "^65.6.0"
types-setuptools = "^65.6.0"
types-redis = "^4.6.0.20240106"
types-pyyaml = "^6.0.12.12"
types-tabulate = "^0.9.0.20240106"
types-waitress = "^2.1.4.20240106"
types-requests = "2.31.0"

[build-system]
requires = ["poetry-core", "wheel"]
build-backend = "poetry.core.masonry.api"

[tool.commitizen]
version = "1.16.19"
version_files = [
    "pyproject.toml:^version"
]

