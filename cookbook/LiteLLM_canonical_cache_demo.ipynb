{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canonical Neural Cache\n",
    "[Canonical Cache](https://canonical.chat/)\n",
    "\n",
    "Email us with any and all questions at <founders@canonical.chat>\n",
    "\n",
    "This is a simple notebook to demonstrate how to use the Canonical Cache with LiteLLM in your LLM apps. An API Key is required, email us at <founders@canonical.chat> to get one. See our GitHub repository [here](https://github.com/Canonical-AI-Inc/canonical) for more examples.\n",
    "\n",
    "## A few important notes:\n",
    "\n",
    " - The first level cache key is a hash of the model's name and system prompt. Changing either of these will automatically create a new cache bucket for you. When you start seeing cache misses after editing your system prompt, this is why.\n",
    "\n",
    " - We enforce a delay period of 5 seconds between when a cache entry is created and when it can be used.\n",
    "\n",
    " - Function calling is not supported and is explicitly ignored by the cache.\n",
    "\n",
    " - Message format must be system -> assistant -> user. See the examples below\n",
    "\n",
    " - When the temperature is > 0, expect to see cache misses initially. We use the temperature to emulate the same kind of behavior you would expect if you weren't using the cache. That is, for temperature values > 0 we build a list of possible completions and then select one at random when the list is full. The greater the temperature, the larger the list and the longer it takes to fill. A concrete example of this when the temperature is 0.1, the list size is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import litellm\n",
    "from litellm.caching import Cache, canonical_cache_key\n",
    "\n",
    "## set ENV variables\n",
    "openaikey = os.environ[\"OPENAI_API_KEY\"]\n",
    "apikey = os.environ[\"CANONICAL_CACHE_API_KEY\"]\n",
    "\n",
    "# The canonical cache only supports completions\n",
    "litellm.cache = Cache(type=\"canonical\",\n",
    "                      apikey=apikey,\n",
    "                      supported_call_types=[\"completion\", \"acompletion\"])\n",
    "\n",
    "# The Canonical cache uses a custom cache key function, set it here\n",
    "litellm.cache.get_cache_key = canonical_cache_key\n",
    "# Off to the races!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will run a syncronous completion. Run it multiple times to see a cache hit. Changing the system prompt will create a new cache key and result in a cache miss. Also, if you change the model name a new cache key will be created, resulting in a cache miss. So, the first level of hashing is a combination of the model name and the system prompt. The next level of hashing is the chat context.\n",
    "\n",
    "Subtle changes to the user's message should still result in a cache hit. For example, if you change `Hello, how are you?` to `Hello, how are you today?` the cache should still hit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from litellm import completion\n",
    "litellm.set_verbose=True\n",
    "\n",
    "messages = [\n",
    "    { \"role\": \"system\", \"content\": \"Be a Gen Z assistant today and everyday.\"},\n",
    "    { \"role\": \"assistant\", \"content\": \"How can I help you today?\" },\n",
    "    { \"role\": \"user\",  \"content\": \"Hello, how are you?\"}\n",
    "]\n",
    "# openai call\n",
    "response = completion(model=\"gpt-3.5-turbo\", messages=messages, stream=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the change to the user's message below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from litellm import completion\n",
    "litellm.set_verbose=True\n",
    "\n",
    "# this subtle change to the user's prompt should result in a cache hit\n",
    "messages = [\n",
    "    { \"role\": \"system\", \"content\": \"Be a Gen Z assistant today and everyday.\"},\n",
    "    { \"role\": \"assistant\", \"content\": \"How can I help you today?\" },\n",
    "    { \"role\": \"user\",  \"content\": \"Hello, how are you today?\"} # notice the change here\n",
    "]\n",
    "# openai call\n",
    "response = completion(model=\"gpt-3.5-turbo\", messages=messages, stream=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below demonstrates an asyncronous completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from litellm import acompletion\n",
    "litellm.set_verbose=True\n",
    "\n",
    "messages = [\n",
    "    { \"role\": \"system\", \"content\": \"Be a Baby Boomer assistant today.\"},\n",
    "    { \"role\": \"assistant\", \"content\": \"How can I help you today?\" },\n",
    "    { \"role\": \"user\",  \"content\": \"Hello, how are you?\"}\n",
    "]\n",
    "\n",
    "# execute an async completion\n",
    "response = await acompletion(model=\"gpt-3.5-turbo\", messages=messages, stream=False)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
