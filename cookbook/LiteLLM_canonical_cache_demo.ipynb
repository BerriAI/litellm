{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A simple notebook to demonstrate how to use the Canonical Cache in your LLM apps. An API Key is required, email us at founders@canonical.chat to get one. For more information, see the [Canonical Cache documentation](https://docs.canonical.chat/) and our GitHub repository [here](https://github.com/Canonical+AI+Inc/canonical) for more examples.\n",
        "\n",
        "A few important notes:\n",
        "1.) The first level cache key is a hash of the model's name and system prompt. So, if you change any of these we will automatically create a new cache bucket for you.\n",
        "\n",
        "2.) We enforce a delay period of 5 seconds between when a cache entry is created and when it can be used.\n",
        "\n",
        "3.) Function calling is not supported and is explicitly ignored by the cache.\n",
        "\n",
        "4.) Message format must be system -> assistant -> user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import litellm\n",
        "from litellm.caching import Cache, canonical_cache_key\n",
        "\n",
        "## set ENV variables\n",
        "openaikey = os.environ[\"OPENAI_API_KEY\"]\n",
        "apikey = os.environ[\"CANONICAL_CACHE_API_KEY\"]\n",
        "\n",
        "# The canonical cache only supports completions\n",
        "litellm.cache = Cache(type=\"canonical\",\n",
        "                      apikey=apikey,\n",
        "                      supported_call_types=[\"completion\", \"acompletion\"])\n",
        "\n",
        "# The Canonical cache uses a custom cache key function, set it here\n",
        "litellm.cache.get_cache_key = canonical_cache_key\n",
        "# Off to the races!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below will run a syncronous completion, run it multiple times to see a cache hit. Changing the system prompt will create a new cache akey and result in a cache miss. Also, if you change the model name a new cache key will be created, resulting in a cache miss. So, the first level of hashing is a combination of the model name and the system prompt. The next level of hashing is the chat context.\n",
        "\n",
        "Subtle changes to the prompt should still result in a cache hit. For example, if you change `Hello, how are you?` to `Hello, how are you today?` the cache should still hit. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import litellm\n",
        "from litellm import completion\n",
        "litellm.set_verbose=True\n",
        "\n",
        "messages = [\n",
        "    { \"role\": \"system\", \"content\": \"Be a Gen Z assistant today and everyday.\"},\n",
        "    { \"role\": \"assistant\", \"content\": \"How can I help you today?\" },\n",
        "    { \"role\": \"user\",  \"content\": \"Hello, how are you?\"}\n",
        "]\n",
        "# openai call\n",
        "response = completion(model=\"gpt-3.5-turbo\", messages=messages, stream=False)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice the change to the user's prompt below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import litellm\n",
        "from litellm import completion\n",
        "litellm.set_verbose=True\n",
        "\n",
        "# this subtle change to the user's prompt should result in a cache hit\n",
        "messages = [\n",
        "    { \"role\": \"system\", \"content\": \"Be a Gen Z assistant today and everyday.\"},\n",
        "    { \"role\": \"assistant\", \"content\": \"How can I help you today?\" },\n",
        "    { \"role\": \"user\",  \"content\": \"Hello, how are you today?\"}\n",
        "]\n",
        "# openai call\n",
        "response = completion(model=\"gpt-3.5-turbo\", messages=messages, stream=False)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The example below demonstrates an asyncronous completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import litellm\n",
        "from litellm import acompletion\n",
        "litellm.set_verbose=True\n",
        "\n",
        "messages = [\n",
        "    { \"role\": \"system\", \"content\": \"Be a Baby Boomer assistant today.\"},\n",
        "    { \"role\": \"assistant\", \"content\": \"How can I help you today?\" },\n",
        "    { \"role\": \"user\",  \"content\": \"Hello, how are you?\"}\n",
        "]\n",
        "\n",
        "# execute an async completion\n",
        "response = await acompletion(model=\"gpt-3.5-turbo\", messages=messages, stream=False)\n",
        "print(response)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
