# Bug Fix: Model Name Parsing in Health Check

## Problem Description

When users configure models in the LiteLLM proxy UI with custom model names containing provider prefixes (e.g., `openai/gpt-oss-20b`, `azure/custom-model`, etc.), the test connection functionality fails with 404 errors.

### Root Cause
The `ahealth_check` function in `litellm/main.py` automatically calls `get_llm_provider(model=model)` which splits model names like `openai/gpt-oss-20b` into:
- `custom_llm_provider = "openai"`  
- `model = "gpt-oss-20b"` (prefix removed)

However, when users explicitly configure OpenAI-compatible endpoints or other custom providers with specific model names, the target server expects to receive the **complete model name** as originally specified by the user.

### Example Failure Case
1. User configures model: `openai/gpt-oss-20b` with `api_base: http://52.192.132.71:8013`
2. Test connection sends request with model: `gpt-oss-20b` (auto-parsed)
3. Target server returns 404 because it only knows `openai/gpt-oss-20b`

## Solution

Modified `ahealth_check` function in `litellm/main.py` (lines 5878-5890) to:

1. **Respect explicit user configuration**: When `model_params` already contains both `custom_llm_provider` and `api_base`, skip automatic model name parsing
2. **Preserve original model names**: Keep the user's complete model specification intact  
3. **Maintain backward compatibility**: Only bypass parsing when user has explicitly configured provider and endpoint

### Root Cause Analysis

The issue occurs because `get_llm_provider()` function is designed to automatically parse model names like `openai/gpt-oss-20b` into:
- Extract provider: `openai`
- Strip model name: `gpt-oss-20b` 

This auto-parsing behavior is correct for most scenarios but breaks when:
- User explicitly configures `custom_llm_provider="openai"` 
- User sets custom `api_base` pointing to their own server
- The target server expects the FULL model name `openai/gpt-oss-20b`

### Code Changes

```python
# Before (automatic parsing always applied)
model, custom_llm_provider, _, _ = get_llm_provider(model=model)

# After (conditional parsing based on explicit configuration)
existing_custom_provider = model_params.get("custom_llm_provider", None)
existing_api_base = model_params.get("api_base", None)

if existing_custom_provider and existing_api_base:
    # User has explicitly configured custom provider and api_base
    # Don't parse the model name to preserve user's intent
    custom_llm_provider = existing_custom_provider
    # Keep the original model name as specified by user
else:
    # Standard behavior: parse model name to determine provider
    model, custom_llm_provider, _, _ = get_llm_provider(model=model)
```

### Why This Fix is Safe

1. **Only affects explicit configurations**: The bypass only applies when both `custom_llm_provider` AND `api_base` are explicitly set
2. **Preserves existing behavior**: All auto-detection scenarios continue to work as before
3. **Matches user intent**: When users explicitly configure provider + endpoint, we respect their complete model specification

## Deep Analysis & Risk Assessment

### Why This Bug Existed

This is a fundamental design issue that affects **multiple critical functions**:

1. **`ahealth_check` is used in TWO scenarios:**
   - üîß **Test Connection** (UI) - Where the bug manifests
   - üîç **Regular Health Checks** - Background monitoring of configured models

2. **`get_llm_provider` is called 45 times across the codebase** but most calls pass existing `custom_llm_provider` parameters correctly

3. **The bug only triggered in `ahealth_check`** because it was the ONLY place calling `get_llm_provider(model=model)` with just the model parameter

### Impact Assessment - Who Gets Affected?

#### ‚úÖ **SAFE - No Impact:** 
- Standard providers (OpenAI, Anthropic, etc.) with official model names
- Auto-detected providers from model prefixes in normal completion calls
- Existing health check configurations in YAML files
- All regular litellm completion/embedding calls

#### ‚ö†Ô∏è **AFFECTED - Now FIXED:**
- OpenAI-compatible endpoints with custom model names like `openai/my-custom-model`
- Azure deployments with custom naming patterns like `azure/my-deployment` 
- Any custom provider using model names with provider prefixes
- **Test Connection functionality in the UI**

#### üéØ **SCENARIOS THAT BENEFIT:**
- Users explicitly configuring `custom_llm_provider="openai"` + custom `api_base`
- Enterprise deployments with custom model naming conventions
- Multi-provider proxy setups with consistent naming

### Risk Mitigation

**Low Risk Change Because:**
1. **Precise condition**: Only applies when BOTH `custom_llm_provider` AND `api_base` are explicitly set
2. **Backward compatible**: Preserves all existing auto-detection behavior  
3. **Matches user intent**: When users explicitly configure provider + endpoint, we respect their model names
4. **Limited scope**: Only affects health check calls, not production LLM calls

## Files Modified

- `litellm/main.py`: Lines 5878-5890 (ahealth_check function)

## Testing Required - Priority Order

1. **HIGH PRIORITY**: Test connection functionality for OpenAI-compatible endpoints
2. **MEDIUM**: Verify existing provider auto-detection still works in health checks
3. **MEDIUM**: Test various custom model name patterns (azure/, anthropic/, etc.)
4. **LOW**: Ensure no regression in standard provider usage in production calls