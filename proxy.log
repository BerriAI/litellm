[92m19:32:14 - LiteLLM Proxy:DEBUG[0m: litellm_license.py:29 - License Str value - None
[92m19:32:14 - LiteLLM Proxy:DEBUG[0m: litellm_license.py:102 - litellm.proxy.auth.litellm_license.py::is_premium() - ENTERING 'IS_PREMIUM' - LiteLLM License=None
[92m19:32:14 - LiteLLM Proxy:DEBUG[0m: litellm_license.py:111 - litellm.proxy.auth.litellm_license.py::is_premium() - Updated 'self.license_str' - None
[92m19:32:14 - LiteLLM:DEBUG[0m: logging_callback_manager.py:279 - Custom logger of type _PROXY_VirtualKeyModelMaxBudgetLimiter, key: _PROXY_VirtualKeyModelMaxBudgetLimiter already exists in [<litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7ff200094050>], not adding again..
INFO:     Started server process [2777]
INFO:     Waiting for application startup.
[92m19:32:14 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:674 - litellm.proxy.proxy_server.py::startup() - CHECKING PREMIUM USER - False
[92m19:32:14 - LiteLLM Proxy:DEBUG[0m: litellm_license.py:102 - litellm.proxy.auth.litellm_license.py::is_premium() - ENTERING 'IS_PREMIUM' - LiteLLM License=None
[92m19:32:14 - LiteLLM Proxy:DEBUG[0m: litellm_license.py:111 - litellm.proxy.auth.litellm_license.py::is_premium() - Updated 'self.license_str' - None
[92m19:32:14 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:687 - worker_config: {"model": null, "alias": null, "api_base": null, "api_version": "2024-07-01-preview", "debug": false, "detailed_debug": true, "temperature": null, "max_tokens": null, "request_timeout": null, "max_budget": null, "telemetry": true, "drop_params": false, "add_function_to_prompt": false, "headers": null, "save": false, "config": "proxy_config.yaml", "use_queue": false}

   ‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ïó
   ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ïë
   ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ñà‚ñà‚ñà‚ñà‚ïî‚ñà‚ñà‚ïë
   ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë
   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë ‚ïö‚ïê‚ïù ‚ñà‚ñà‚ïë
   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù

[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': '5583ac0c3e38cfd381b6cc09bcca6e0db60af48d3f16da325f82eb9df1b6a1e4', 'combined_model_name': '5583ac0c3e38cfd381b6cc09bcca6e0db60af48d3f16da325f82eb9df1b6a1e4', 'stripped_model_name': '5583ac0c3e38cfd381b6cc09bcca6e0db60af48d3f16da325f82eb9df1b6a1e4', 'combined_stripped_model_name': '5583ac0c3e38cfd381b6cc09bcca6e0db60af48d3f16da325f82eb9df1b6a1e4', 'custom_llm_provider': None}
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:5118 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:2360 - added/updated model=5583ac0c3e38cfd381b6cc09bcca6e0db60af48d3f16da325f82eb9df1b6a1e4 in litellm.model_cost: 5583ac0c3e38cfd381b6cc09bcca6e0db60af48d3f16da325f82eb9df1b6a1e4
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': '*', 'combined_model_name': 'openai/*', 'stripped_model_name': 'openai/*', 'combined_stripped_model_name': 'openai/*', 'custom_llm_provider': 'openai'}
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:5118 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:2360 - added/updated model=openai/* in litellm.model_cost: openai/*
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': 'e0f302a1412e78470ebb28cbed01fff5f88c0d331c667e9f2ba4b413c6fbd282', 'combined_model_name': 'e0f302a1412e78470ebb28cbed01fff5f88c0d331c667e9f2ba4b413c6fbd282', 'stripped_model_name': 'e0f302a1412e78470ebb28cbed01fff5f88c0d331c667e9f2ba4b413c6fbd282', 'combined_stripped_model_name': 'e0f302a1412e78470ebb28cbed01fff5f88c0d331c667e9f2ba4b413c6fbd282', 'custom_llm_provider': None}
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:5118 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:2360 - added/updated model=e0f302a1412e78470ebb28cbed01fff5f88c0d331c667e9f2ba4b413c6fbd282 in litellm.model_cost: e0f302a1412e78470ebb28cbed01fff5f88c0d331c667e9f2ba4b413c6fbd282
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': '*', 'combined_model_name': 'anthropic/*', 'stripped_model_name': '*', 'combined_stripped_model_name': 'anthropic/*', 'custom_llm_provider': 'anthropic'}
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:5118 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:2131 - Model not found or error in checking supports_reasoning support. You passed model=*, custom_llm_provider=anthropic. Error: This model isn't mapped yet. model=*, custom_llm_provider=anthropic. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': '*', 'combined_model_name': 'anthropic/*', 'stripped_model_name': 'anthropic/*', 'combined_stripped_model_name': 'anthropic/*', 'custom_llm_provider': 'anthropic'}
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:5118 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m19:32:14 - LiteLLM:DEBUG[0m: utils.py:2360 - added/updated model=anthropic/* in litellm.model_cost: anthropic/*
[92m19:32:14 - LiteLLM Router:DEBUG[0m: router.py:5585 - 
Initialized Model List ['openai/*', 'anthropic/*']
[92m19:32:14 - LiteLLM Router:INFO[0m: router.py:711 - Routing strategy: simple-shuffle
[92m19:32:14 - LiteLLM:DEBUG[0m: logging_callback_manager.py:279 - Custom logger of type _PROXY_VirtualKeyModelMaxBudgetLimiter, key: _PROXY_VirtualKeyModelMaxBudgetLimiter already exists in [<litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7ff200094050>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7ff1fe723110>, <litellm.proxy.hooks.parallel_request_limiter_v3._PROXY_MaxParallelRequestsHandler_v3 object at 0x7ff1fe7aa120>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7ff1fe722ad0>, <litellm.proxy.hooks.responses_id_security.ResponsesIDSecurity object at 0x7ff1fe7aa270>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x7ff1fe7aa3c0>, <litellm._service_logger.ServiceLogging object at 0x7ff1ff2f1ba0>], not adding again..
[92m19:32:14 - LiteLLM:DEBUG[0m: logging_callback_manager.py:279 - Custom logger of type _PROXY_MaxBudgetLimiter, key: _PROXY_MaxBudgetLimiter already exists in [<litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7ff200094050>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7ff1fe723110>, <litellm.proxy.hooks.parallel_request_limiter_v3._PROXY_MaxParallelRequestsHandler_v3 object at 0x7ff1fe7aa120>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7ff1fe722ad0>, <litellm.proxy.hooks.responses_id_security.ResponsesIDSecurity object at 0x7ff1fe7aa270>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x7ff1fe7aa3c0>, <litellm._service_logger.ServiceLogging object at 0x7ff1ff2f1ba0>], not adding again..
[92m19:32:14 - LiteLLM:DEBUG[0m: logging_callback_manager.py:279 - Custom logger of type _PROXY_MaxParallelRequestsHandler_v3, key: _PROXY_MaxParallelRequestsHandler_v3-window_size=60 already exists in [<litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7ff200094050>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7ff1fe723110>, <litellm.proxy.hooks.parallel_request_limiter_v3._PROXY_MaxParallelRequestsHandler_v3 object at 0x7ff1fe7aa120>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7ff1fe722ad0>, <litellm.proxy.hooks.responses_id_security.ResponsesIDSecurity object at 0x7ff1fe7aa270>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x7ff1fe7aa3c0>, <litellm._service_logger.ServiceLogging object at 0x7ff1ff2f1ba0>], not adding again..
[92m19:32:14 - LiteLLM:DEBUG[0m: logging_callback_manager.py:279 - Custom logger of type _PROXY_CacheControlCheck, key: _PROXY_CacheControlCheck already exists in [<litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7ff200094050>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7ff1fe723110>, <litellm.proxy.hooks.parallel_request_limiter_v3._PROXY_MaxParallelRequestsHandler_v3 object at 0x7ff1fe7aa120>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7ff1fe722ad0>, <litellm.proxy.hooks.responses_id_security.ResponsesIDSecurity object at 0x7ff1fe7aa270>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x7ff1fe7aa3c0>, <litellm._service_logger.ServiceLogging object at 0x7ff1ff2f1ba0>], not adding again..
[92m19:32:14 - LiteLLM:DEBUG[0m: logging_callback_manager.py:279 - Custom logger of type ResponsesIDSecurity, key: ResponsesIDSecurity already exists in [<litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7ff200094050>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7ff1fe723110>, <litellm.proxy.hooks.parallel_request_limiter_v3._PROXY_MaxParallelRequestsHandler_v3 object at 0x7ff1fe7aa120>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7ff1fe722ad0>, <litellm.proxy.hooks.responses_id_security.ResponsesIDSecurity object at 0x7ff1fe7aa270>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x7ff1fe7aa3c0>, <litellm._service_logger.ServiceLogging object at 0x7ff1ff2f1ba0>], not adding again..
[92m19:32:14 - LiteLLM:DEBUG[0m: logging_callback_manager.py:279 - Custom logger of type _PROXY_LiteLLMManagedFiles, key: _PROXY_LiteLLMManagedFiles already exists in [<litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7ff200094050>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7ff1fe723110>, <litellm.proxy.hooks.parallel_request_limiter_v3._PROXY_MaxParallelRequestsHandler_v3 object at 0x7ff1fe7aa120>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7ff1fe722ad0>, <litellm.proxy.hooks.responses_id_security.ResponsesIDSecurity object at 0x7ff1fe7aa270>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x7ff1fe7aa3c0>, <litellm._service_logger.ServiceLogging object at 0x7ff1ff2f1ba0>], not adding again..
[92m19:32:14 - LiteLLM:DEBUG[0m: logging_callback_manager.py:279 - Custom logger of type ServiceLogging, key: ServiceLogging-mock_testing=False-mock_testing_sync_success_hook=0-mock_testing_async_success_hook=0-mock_testing_sync_failure_hook=0-mock_testing_async_failure_hook=0 already exists in [<litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7ff200094050>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7ff1fe723110>, <litellm.proxy.hooks.parallel_request_limiter_v3._PROXY_MaxParallelRequestsHandler_v3 object at 0x7ff1fe7aa120>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7ff1fe722ad0>, <litellm.proxy.hooks.responses_id_security.ResponsesIDSecurity object at 0x7ff1fe7aa270>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x7ff1fe7aa3c0>, <litellm._service_logger.ServiceLogging object at 0x7ff1ff2f1ba0>], not adding again..
[92m19:32:14 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:756 - prisma_client: None
[92m19:32:14 - LiteLLM Proxy:INFO[0m: proxy_server.py:655 - SESSION REUSE: Created shared aiohttp session for connection pooling (ID: 140677333297408, limit=300, limit_per_host=50)
[92m19:32:14 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)

[1;37m#------------------------------------------------------------#[0m
[1;37m#                                                            #[0m
[1;37m#              'I don't like how this works...'               #[0m
[1;37m#        https://github.com/BerriAI/litellm/issues/new        #[0m
[1;37m#                                                            #[0m
[1;37m#------------------------------------------------------------#[0m

 Thank you for using LiteLLM! - Krrish & Ishaan



[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m


[32mLiteLLM: Proxy initialized with Config, Set models:[0m
[32m    openai/*[0m
[32m    anthropic/*[0m
INFO:     127.0.0.1:56074 - "GET /health/liveness HTTP/1.1" 200 OK
[92m19:32:15 - LiteLLM Proxy:DEBUG[0m: http_parsing_utils.py:398 - populate_request_with_path_params: No vector_store_id present in path=/chat/completions
[92m19:32:15 - LiteLLM Proxy:DEBUG[0m: common_request_processing.py:486 - Request received by LiteLLM:
{
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful coding assistant."
        },
        {
            "role": "user",
            "content": "Please edit the file /workspace/test.txt. Change 'hello' to 'world'."
        }
    ],
    "model": "anthropic/claude-3-opus-20240229",
    "tool_choice": "auto",
    "tools": [
        {
            "type": "function",
            "function": {
                "name": "edit_file",
                "description": "Edit a file in the workspace",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "path": {
                            "type": "string"
                        },
                        "old_string": {
                            "type": "string"
                        },
                        "new_string": {
                            "type": "string"
                        }
                    },
                    "required": [
                        "path",
                        "old_string",
                        "new_string"
                    ]
                }
            }
        }
    ]
}
[92m19:32:15 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:884 - Request Headers: Headers({'host': '0.0.0.0:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 2.8.0', 'x-stainless-lang': 'python', 'x-stainless-package-version': '2.8.0', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.13.3', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600', 'content-length': '527'})
[92m19:32:15 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:890 - receiving data: {'messages': [{'role': 'system', 'content': 'You are a helpful coding assistant.'}, {'role': 'user', 'content': "Please edit the file /workspace/test.txt. Change 'hello' to 'world'."}], 'model': 'anthropic/claude-3-opus-20240229', 'tool_choice': 'auto', 'tools': [{'type': 'function', 'function': {'name': 'edit_file', 'description': 'Edit a file in the workspace', 'parameters': {'type': 'object', 'properties': {'path': {'type': 'string'}, 'old_string': {'type': 'string'}, 'new_string': {'type': 'string'}}, 'required': ['path', 'old_string', 'new_string']}}}], 'proxy_server_request': {'url': 'http://0.0.0.0:4000/chat/completions', 'method': 'POST', 'headers': {'host': '0.0.0.0:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 2.8.0', 'x-stainless-lang': 'python', 'x-stainless-package-version': '2.8.0', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.13.3', 'x-stainless-async': 'false', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600', 'content-length': '527'}, 'body': {'messages': [{'role': 'system', 'content': 'You are a helpful coding assistant.'}, {'role': 'user', 'content': "Please edit the file /workspace/test.txt. Change 'hello' to 'world'."}], 'model': 'anthropic/claude-3-opus-20240229', 'tool_choice': 'auto', 'tools': [{'type': 'function', 'function': {'name': 'edit_file', 'description': 'Edit a file in the workspace', 'parameters': {'type': 'object', 'properties': {'path': {'type': 'string'}, 'old_string': {'type': 'string'}, 'new_string': {'type': 'string'}}, 'required': ['path', 'old_string', 'new_string']}}}]}}, 'metadata': {}, 'secret_fields': {'raw_headers': {'host': '0.0.0.0:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 2.8.0', 'x-stainless-lang': 'python', 'x-stainless-package-version': '2.8.0', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.13.3', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600', 'content-length': '527'}}}
[92m19:32:15 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:1091 - [PROXY] returned data from litellm_pre_call_utils: {'messages': [{'role': 'system', 'content': 'You are a helpful coding assistant.'}, {'role': 'user', 'content': "Please edit the file /workspace/test.txt. Change 'hello' to 'world'."}], 'model': 'anthropic/claude-3-opus-20240229', 'tool_choice': 'auto', 'tools': [{'type': 'function', 'function': {'name': 'edit_file', 'description': 'Edit a file in the workspace', 'parameters': {'type': 'object', 'properties': {'path': {'type': 'string'}, 'old_string': {'type': 'string'}, 'new_string': {'type': 'string'}}, 'required': ['path', 'old_string', 'new_string']}}}], 'proxy_server_request': {'url': 'http://0.0.0.0:4000/chat/completions', 'method': 'POST', 'headers': {'host': '0.0.0.0:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 2.8.0', 'x-stainless-lang': 'python', 'x-stainless-package-version': '2.8.0', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.13.3', 'x-stainless-async': 'false', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600', 'content-length': '527'}, 'body': {'messages': [{'role': 'system', 'content': 'You are a helpful coding assistant.'}, {'role': 'user', 'content': "Please edit the file /workspace/test.txt. Change 'hello' to 'world'."}], 'model': 'anthropic/claude-3-opus-20240229', 'tool_choice': 'auto', 'tools': [{'type': 'function', 'function': {'name': 'edit_file', 'description': 'Edit a file in the workspace', 'parameters': {'type': 'object', 'properties': {'path': {'type': 'string'}, 'old_string': {'type': 'string'}, 'new_string': {'type': 'string'}}, 'required': ['path', 'old_string', 'new_string']}}}]}}, 'metadata': {'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/chat/completions', 'user_api_key_budget_reset_at': None, 'user_api_key_auth_metadata': {}, 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'user_api_key_auth': UserAPIKeyAuth(token='88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', key_name=None, key_alias=None, spend=0.0, max_budget=None, expires=None, models=[], aliases={}, config={}, user_id=None, team_id=None, max_parallel_requests=None, metadata={}, tpm_limit=None, rpm_limit=None, budget_duration=None, budget_reset_at=None, allowed_cache_controls=[], allowed_routes=[], permissions={}, model_spend={}, model_max_budget={}, soft_budget_cooldown=False, blocked=None, litellm_budget_table=None, org_id=None, created_at=None, created_by=None, updated_at=None, updated_by=None, object_permission_id=None, object_permission=None, rotation_count=0, auto_rotate=False, rotation_interval=None, last_rotation_at=None, key_rotation_at=None, team_spend=None, team_alias=None, team_tpm_limit=None, team_rpm_limit=None, team_max_budget=None, team_models=[], team_blocked=False, soft_budget=None, team_model_aliases=None, team_member=None, team_metadata=None, team_object_permission_id=None, team_member_spend=None, team_member_tpm_limit=None, team_member_rpm_limit=None, end_user_id=None, end_user_tpm_limit=None, end_user_rpm_limit=None, end_user_max_budget=None, organization_max_budget=None, organization_tpm_limit=None, organization_rpm_limit=None, organization_metadata=None, last_refreshed_at=None, api_key='88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', user_role=<LitellmUserRoles.PROXY_ADMIN: 'proxy_admin'>, allowed_model_region=None, parent_otel_span=None, rpm_limit_per_model=None, tpm_limit_per_model=None, user_tpm_limit=None, user_rpm_limit=None, user_email=None, request_route='/chat/completions'), 'litellm_api_version': '1.80.10', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': '0.0.0.0:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 2.8.0', 'x-stainless-lang': 'python', 'x-stainless-package-version': '2.8.0', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.13.3', 'x-stainless-async': 'false', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600', 'content-length': '527'}, 'endpoint': 'http://0.0.0.0:4000/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': ''}, 'secret_fields': {'raw_headers': {'host': '0.0.0.0:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 2.8.0', 'x-stainless-lang': 'python', 'x-stainless-package-version': '2.8.0', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.13.3', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600', 'content-length': '527'}}}
[92m19:32:15 - LiteLLM:DEBUG[0m: utils.py:385 - Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7ff1fe7a97f0>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7ff200094050>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7ff1fe723110>, <litellm.proxy.hooks.parallel_request_limiter_v3._PROXY_MaxParallelRequestsHandler_v3 object at 0x7ff1fe7aa120>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7ff1fe722ad0>, <litellm.proxy.hooks.responses_id_security.ResponsesIDSecurity object at 0x7ff1fe7aa270>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x7ff1fe7aa3c0>, <litellm._service_logger.ServiceLogging object at 0x7ff1ff2f1ba0>]
[92m19:32:15 - LiteLLM:DEBUG[0m: litellm_logging.py:514 - self.optional_params: {}
[92m19:32:15 - LiteLLM Proxy:DEBUG[0m: utils.py:959 - Inside Proxy Logging Pre-call hook!
[92m19:32:15 - LiteLLM Proxy:DEBUG[0m: max_budget_limiter.py:23 - Inside Max Budget Limiter Pre-Call Hook
[92m19:32:15 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter_v3.py:1134 - Inside Rate Limit Pre-Call Hook
[92m19:32:15 - LiteLLM Proxy:DEBUG[0m: cache_control_check.py:27 - Inside Cache Control Check Pre-Call Hook
[92m19:32:15 - LiteLLM Router:DEBUG[0m: router.py:4401 - Inside async function with retries.
[92m19:32:15 - LiteLLM Router:DEBUG[0m: router.py:4421 - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x7ff1fe7a97f0>>, num_retries - 2
[92m19:32:15 - LiteLLM Router:DEBUG[0m: router.py:7564 - healthy_deployments after team filter: [{'model_name': 'anthropic/*', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'anthropic/claude-3-opus-20240229'}, 'model_info': {'id': 'e0f302a1412e78470ebb28cbed01fff5f88c0d331c667e9f2ba4b413c6fbd282', 'db_model': False}}]
[92m19:32:15 - LiteLLM Router:DEBUG[0m: router.py:7571 - healthy_deployments after web search filter: [{'model_name': 'anthropic/*', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'anthropic/claude-3-opus-20240229'}, 'model_info': {'id': 'e0f302a1412e78470ebb28cbed01fff5f88c0d331c667e9f2ba4b413c6fbd282', 'db_model': False}}]
[92m19:32:15 - LiteLLM Router:DEBUG[0m: cooldown_handlers.py:344 - retrieve cooldown models: []
[92m19:32:15 - LiteLLM Router:DEBUG[0m: router.py:7579 - async cooldown deployments: []
[92m19:32:15 - LiteLLM Router:DEBUG[0m: router.py:7582 - cooldown_deployments: []
[92m19:32:15 - LiteLLM Router:DEBUG[0m: router.py:7947 - cooldown deployments: []
[92m19:32:15 - LiteLLM:DEBUG[0m: utils.py:385 - 

[92m19:32:15 - LiteLLM:DEBUG[0m: utils.py:385 - [92mRequest to litellm:[0m
[92m19:32:15 - LiteLLM:DEBUG[0m: utils.py:385 - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='anthropic/claude-3-opus-20240229', messages=[{'role': 'system', 'content': 'You are a helpful coding assistant.'}, {'role': 'user', 'content': "Please edit the file /workspace/test.txt. Change 'hello' to 'world'."}], caching=False, client=None, tool_choice='auto', tools=[{'type': 'function', 'function': {'name': 'edit_file', 'description': 'Edit a file in the workspace', 'parameters': {'type': 'object', 'properties': {'path': {'type': 'string'}, 'old_string': {'type': 'string'}, 'new_string': {'type': 'string'}}, 'required': ['path', 'old_string', 'new_string']}}}], proxy_server_request={'url': 'http://0.0.0.0:4000/chat/completions', 'method': 'POST', 'headers': {'host': '0.0.0.0:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 2.8.0', 'x-stainless-lang': 'python', 'x-stainless-package-version': '2.8.0', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.13.3', 'x-stainless-async': 'false', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600', 'content-length': '527'}, 'body': {'messages': [{'role': 'system', 'content': 'You are a helpful coding assistant.'}, {'role': 'user', 'content': "Please edit the file /workspace/test.txt. Change 'hello' to 'world'."}], 'model': 'anthropic/claude-3-opus-20240229', 'tool_choice': 'auto', 'tools': [{'type': 'function', 'function': {'name': 'edit_file', 'description': 'Edit a file in the workspace', 'parameters': {'type': 'object', 'properties': {'path': {'type': 'string'}, 'old_string': {'type': 'string'}, 'new_string': {'type': 'string'}}, 'required': ['path', 'old_string', 'new_string']}}}]}}, metadata={'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/chat/completions', 'user_api_key_budget_reset_at': None, 'user_api_key_auth_metadata': {}, 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'user_api_key_auth': UserAPIKeyAuth(token='88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', key_name=None, key_alias=None, spend=0.0, max_budget=None, expires=None, models=[], aliases={}, config={}, user_id=None, team_id=None, max_parallel_requests=None, metadata={}, tpm_limit=None, rpm_limit=None, budget_duration=None, budget_reset_at=None, allowed_cache_controls=[], allowed_routes=[], permissions={}, model_spend={}, model_max_budget={}, soft_budget_cooldown=False, blocked=None, litellm_budget_table=None, org_id=None, created_at=None, created_by=None, updated_at=None, updated_by=None, object_permission_id=None, object_permission=None, rotation_count=0, auto_rotate=False, rotation_interval=None, last_rotation_at=None, key_rotation_at=None, team_spend=None, team_alias=None, team_tpm_limit=None, team_rpm_limit=None, team_max_budget=None, team_models=[], team_blocked=False, soft_budget=None, team_model_aliases=None, team_member=None, team_metadata=None, team_object_permission_id=None, team_member_spend=None, team_member_tpm_limit=None, team_member_rpm_limit=None, end_user_id=None, end_user_tpm_limit=None, end_user_rpm_limit=None, end_user_max_budget=None, organization_max_budget=None, organization_tpm_limit=None, organization_rpm_limit=None, organization_metadata=None, last_refreshed_at=None, api_key='88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', user_role=<LitellmUserRoles.PROXY_ADMIN: 'proxy_admin'>, allowed_model_region=None, parent_otel_span=None, rpm_limit_per_model=None, tpm_limit_per_model=None, user_tpm_limit=None, user_rpm_limit=None, user_email=None, request_route='/chat/completions'), 'litellm_api_version': '1.80.10', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': '0.0.0.0:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 2.8.0', 'x-stainless-lang': 'python', 'x-stainless-package-version': '2.8.0', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.13.3', 'x-stainless-async': 'false', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600', 'content-length': '527'}, 'endpoint': 'http://0.0.0.0:4000/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': '', 'model_group': 'anthropic/claude-3-opus-20240229', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'anthropic/claude-3-opus-20240229', 'model_info': {'id': 'e0f302a1412e78470ebb28cbed01fff5f88c0d331c667e9f2ba4b413c6fbd282', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'anthropic/*', 'caching_groups': None}, secret_fields={'raw_headers': {'host': '0.0.0.0:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 2.8.0', 'x-stainless-lang': 'python', 'x-stainless-package-version': '2.8.0', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.13.3', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600', 'content-length': '527'}}, litellm_call_id='a87b2d79-18ab-4991-a183-e6e50557ea65', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x7ff1fe7ab8c0>, shared_session=<aiohttp.client.ClientSession object at 0x7ff1fe7aa900>, stream=False, litellm_trace_id='737fb079-fb57-4422-998c-1e0e3dbd1de0', model_info={'id': 'e0f302a1412e78470ebb28cbed01fff5f88c0d331c667e9f2ba4b413c6fbd282', 'db_model': False}, timeout=6000.0, max_retries=0)[0m
[92m19:32:15 - LiteLLM:DEBUG[0m: utils.py:385 - 

[92m19:32:15 - LiteLLM:DEBUG[0m: utils.py:385 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
[92m19:32:15 - LiteLLM:DEBUG[0m: main.py:509 - üîÑ SHARED SESSION: acompletion called with shared_session (ID: 140677333297408)
[92m19:32:15 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-opus-20240229', 'combined_model_name': 'anthropic/claude-3-opus-20240229', 'stripped_model_name': 'claude-3-opus-20240229', 'combined_stripped_model_name': 'anthropic/claude-3-opus-20240229', 'custom_llm_provider': 'anthropic'}
[92m19:32:15 - LiteLLM:INFO[0m: utils.py:3445 - 
LiteLLM completion() model= claude-3-opus-20240229; provider = anthropic
[92m19:32:15 - LiteLLM:DEBUG[0m: utils.py:3448 - 
LiteLLM: Params passed to completion() {'model': 'claude-3-opus-20240229', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'anthropic', 'response_format': None, 'seed': None, 'tools': [{'type': 'function', 'function': {'name': 'edit_file', 'description': 'Edit a file in the workspace', 'parameters': {'type': 'object', 'properties': {'path': {'type': 'string'}, 'old_string': {'type': 'string'}, 'new_string': {'type': 'string'}}, 'required': ['path', 'old_string', 'new_string']}}}], 'tool_choice': 'auto', 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are a helpful coding assistant.'}, {'role': 'user', 'content': "Please edit the file /workspace/test.txt. Change 'hello' to 'world'."}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
[92m19:32:15 - LiteLLM:DEBUG[0m: utils.py:3451 - 
LiteLLM: Non-Default params passed to completion() {'stream': False, 'tools': [{'type': 'function', 'function': {'name': 'edit_file', 'description': 'Edit a file in the workspace', 'parameters': {'type': 'object', 'properties': {'path': {'type': 'string'}, 'old_string': {'type': 'string'}, 'new_string': {'type': 'string'}}, 'required': ['path', 'old_string', 'new_string']}}}], 'tool_choice': 'auto', 'max_retries': 0}
[92m19:32:15 - LiteLLM:DEBUG[0m: utils.py:385 - Final returned optional params: {'tools': [{'name': 'edit_file', 'input_schema': {'type': 'object', 'properties': {'path': {'type': 'string'}, 'old_string': {'type': 'string'}, 'new_string': {'type': 'string'}}, 'required': ['path', 'old_string', 'new_string']}, 'description': 'Edit a file in the workspace'}], 'tool_choice': {'type': 'auto'}}
[92m19:32:15 - LiteLLM:DEBUG[0m: litellm_logging.py:514 - self.optional_params: {'stream': False, 'tools': [{'type': 'function', 'function': {'name': 'edit_file', 'description': 'Edit a file in the workspace', 'parameters': {'type': 'object', 'properties': {'path': {'type': 'string'}, 'old_string': {'type': 'string'}, 'new_string': {'type': 'string'}}, 'required': ['path', 'old_string', 'new_string']}}}], 'tool_choice': 'auto', 'max_retries': 0}
[92m19:32:15 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-opus-20240229', 'combined_model_name': 'anthropic/claude-3-opus-20240229', 'stripped_model_name': 'claude-3-opus-20240229', 'combined_stripped_model_name': 'anthropic/claude-3-opus-20240229', 'custom_llm_provider': 'anthropic'}
[92m19:32:15 - LiteLLM:DEBUG[0m: litellm_logging.py:1053 - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://api.anthropic.com/v1/messages \
-H 'anthropic-version: 2023-06-01' -H 'x-api-key: sk****AA' -H 'accept: application/json' -H 'content-type: application/json' \
-d '{'model': 'claude-3-opus-20240229', 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': "Please edit the file /workspace/test.txt. Change 'hello' to 'world'."}]}], 'tools': [{'name': 'edit_file', 'input_schema': {'type': 'object', 'properties': {'path': {'type': 'string'}, 'old_string': {'type': 'string'}, 'new_string': {'type': 'string'}}, 'required': ['path', 'old_string', 'new_string']}, 'description': 'Edit a file in the workspace'}], 'tool_choice': {'type': 'auto'}, 'system': [{'type': 'text', 'text': 'You are a helpful coding assistant.'}], 'max_tokens': 4096}'
[0m

[92m19:32:15 - LiteLLM:DEBUG[0m: main.py:6561 - _is_function_call: False
[92m19:32:15 - LiteLLM:DEBUG[0m: litellm_logging.py:1126 - RAW RESPONSE:
<coroutine object AnthropicChatCompletion.acompletion_function at 0x7ff2003eba60>


[92m19:32:15 - LiteLLM:DEBUG[0m: http_handler.py:745 - Using AiohttpTransport...
[92m19:32:15 - LiteLLM:DEBUG[0m: http_handler.py:803 - Creating AiohttpTransport...
[92m19:32:15 - LiteLLM:DEBUG[0m: http_handler.py:813 - NEW SESSION: Creating new ClientSession (no shared session provided)
[92m19:32:22 - LiteLLM:DEBUG[0m: logging_utils.py:165 - `logging_obj` not found - unable to track `llm_api_duration_ms
[92m19:32:22 - LiteLLM:DEBUG[0m: litellm_logging.py:1126 - RAW RESPONSE:
{"model":"claude-3-opus-20240229","id":"msg_01LcvU6MezjwJ3A3CZutvgLY","type":"message","role":"assistant","content":[{"type":"text","text":"<thinking>\nThe relevant tool to use here is edit_file. It allows editing a file in the workspace by replacing an old string with a new string.\n\nThe edit_file tool requires 3 parameters:\npath: The user specified the path as /workspace/test.txt\nold_string: The user wants to change 'hello'  \nnew_string: The user wants to change it to 'world'\n\nThe user has provided all the necessary information to call the edit_file tool. No other tools are needed.\n</thinking>"},{"type":"tool_use","id":"toolu_0143pBK9f4Hc812qTJ31xUvU","name":"edit_file","input":{"path":"/workspace/test.txt","old_string":"hello","new_string":"world"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":657,"cache_creation_input_tokens":0,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":0},"output_tokens":212,"service_tier":"standard"}}


[92m19:32:22 - LiteLLM:DEBUG[0m: litellm_logging.py:3083 - Filtered callbacks: []
[92m19:32:22 - LiteLLM:DEBUG[0m: cost_calculator.py:870 - selected model name for cost calculation: anthropic/claude-3-opus-20240229
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-opus-20240229', 'combined_model_name': 'anthropic/claude-3-opus-20240229', 'stripped_model_name': 'claude-3-opus-20240229', 'combined_stripped_model_name': 'anthropic/claude-3-opus-20240229', 'custom_llm_provider': 'anthropic'}
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-opus-20240229', 'combined_model_name': 'anthropic/claude-3-opus-20240229', 'stripped_model_name': 'claude-3-opus-20240229', 'combined_stripped_model_name': 'anthropic/claude-3-opus-20240229', 'custom_llm_provider': 'anthropic'}
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:5208 - model_info: {'key': 'claude-3-opus-20240229', 'max_tokens': 4096, 'max_input_tokens': 200000, 'max_output_tokens': 4096, 'input_cost_per_token': 1.5e-05, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': 1.875e-05, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.5e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': 6e-06, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.5e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'anthropic', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': True, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
[92m19:32:22 - LiteLLM:DEBUG[0m: litellm_logging.py:1407 - response_cost: 0.025755
[92m19:32:22 - LiteLLM Router:INFO[0m: router.py:1553 - litellm.acompletion(model=anthropic/claude-3-opus-20240229)[32m 200 OK[0m
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-opus-20240229', 'combined_model_name': 'anthropic/claude-3-opus-20240229', 'stripped_model_name': 'claude-3-opus-20240229', 'combined_stripped_model_name': 'anthropic/claude-3-opus-20240229', 'custom_llm_provider': 'anthropic'}
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-opus-20240229', 'combined_model_name': 'anthropic/claude-3-opus-20240229', 'stripped_model_name': 'anthropic/claude-3-opus-20240229', 'combined_stripped_model_name': 'anthropic/claude-3-opus-20240229', 'custom_llm_provider': 'anthropic'}
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:5208 - model_info: {'key': 'claude-3-opus-20240229', 'max_tokens': 4096, 'max_input_tokens': 200000, 'max_output_tokens': 4096, 'input_cost_per_token': 1.5e-05, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': 1.875e-05, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.5e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': 6e-06, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.5e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'anthropic', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': True, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
[92m19:32:22 - LiteLLM Router:DEBUG[0m: router.py:4334 - Async Response: ModelResponse(id='chatcmpl-ff993641-c234-4499-8b06-dff2da9cd4f6', created=1765654342, model='claude-3-opus-20240229', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content="<thinking>\nThe relevant tool to use here is edit_file. It allows editing a file in the workspace by replacing an old string with a new string.\n\nThe edit_file tool requires 3 parameters:\npath: The user specified the path as /workspace/test.txt\nold_string: The user wants to change 'hello'  \nnew_string: The user wants to change it to 'world'\n\nThe user has provided all the necessary information to call the edit_file tool. No other tools are needed.\n</thinking>", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{"path": "/workspace/test.txt", "old_string": "hello", "new_string": "world"}', name='edit_file'), id='toolu_0143pBK9f4Hc812qTJ31xUvU', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=212, prompt_tokens=657, total_tokens=869, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0))
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:385 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x7ff1fe7ab8c0>>
[92m19:32:22 - LiteLLM:DEBUG[0m: litellm_logging.py:3083 - Filtered callbacks: []
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:385 - Logging Details LiteLLM-Async Success Call, cache_hit=None
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-opus-20240229', 'combined_model_name': 'anthropic/claude-3-opus-20240229', 'stripped_model_name': 'claude-3-opus-20240229', 'combined_stripped_model_name': 'anthropic/claude-3-opus-20240229', 'custom_llm_provider': 'anthropic'}
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-opus-20240229', 'combined_model_name': 'anthropic/claude-3-opus-20240229', 'stripped_model_name': 'claude-3-opus-20240229', 'combined_stripped_model_name': 'anthropic/claude-3-opus-20240229', 'custom_llm_provider': 'anthropic'}
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:5208 - model_info: {'key': 'claude-3-opus-20240229', 'max_tokens': 4096, 'max_input_tokens': 200000, 'max_output_tokens': 4096, 'input_cost_per_token': 1.5e-05, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': 1.875e-05, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.5e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': 6e-06, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.5e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'anthropic', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': True, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:385 - Async success callbacks: Got a complete streaming response
[92m19:32:22 - LiteLLM:DEBUG[0m: litellm_logging.py:2395 - Model=claude-3-opus-20240229; cost=0.025755
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-opus-20240229', 'combined_model_name': 'anthropic/claude-3-opus-20240229', 'stripped_model_name': 'claude-3-opus-20240229', 'combined_stripped_model_name': 'anthropic/claude-3-opus-20240229', 'custom_llm_provider': 'anthropic'}
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-opus-20240229', 'combined_model_name': 'anthropic/claude-3-opus-20240229', 'stripped_model_name': 'claude-3-opus-20240229', 'combined_stripped_model_name': 'anthropic/claude-3-opus-20240229', 'custom_llm_provider': 'anthropic'}
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:5208 - model_info: {'key': 'claude-3-opus-20240229', 'max_tokens': 4096, 'max_input_tokens': 200000, 'max_output_tokens': 4096, 'input_cost_per_token': 1.5e-05, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': 1.875e-05, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.5e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': 6e-06, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.5e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'anthropic', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': True, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7ff1fe7a97f0>> is disabled via headers. Disable callbacks from headers: None
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-opus-20240229', 'combined_model_name': 'anthropic/claude-3-opus-20240229', 'stripped_model_name': 'claude-3-opus-20240229', 'combined_stripped_model_name': 'anthropic/claude-3-opus-20240229', 'custom_llm_provider': 'anthropic'}
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:4867 - checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-opus-20240229', 'combined_model_name': 'anthropic/claude-3-opus-20240229', 'stripped_model_name': 'anthropic/claude-3-opus-20240229', 'combined_stripped_model_name': 'anthropic/claude-3-opus-20240229', 'custom_llm_provider': 'anthropic'}
[92m19:32:22 - LiteLLM:DEBUG[0m: utils.py:5208 - model_info: {'key': 'claude-3-opus-20240229', 'max_tokens': 4096, 'max_input_tokens': 200000, 'max_output_tokens': 4096, 'input_cost_per_token': 1.5e-05, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': 1.875e-05, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.5e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': 6e-06, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.5e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'anthropic', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': True, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7ff200094050> is disabled via headers. Disable callbacks from headers: None
[92m19:32:22 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:151 - in RouterBudgetLimiting.async_log_success_event
[92m19:32:22 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:167 - Not running _PROXY_VirtualKeyModelMaxBudgetLimiter.async_log_success_event because user_api_key_model_max_budget is None or empty. `user_api_key_model_max_budget`={}
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7ff1fe723110> is disabled via headers. Disable callbacks from headers: None
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter_v3._PROXY_MaxParallelRequestsHandler_v3 object at 0x7ff1fe7aa120> is disabled via headers. Disable callbacks from headers: None
[92m19:32:22 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter_v3.py:1368 - INSIDE parallel request limiter ASYNC SUCCESS LOGGING
[92m19:32:22 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter_v3.py:1311 - TTL preservation script not available, using regular pipeline
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7ff1fe722ad0> is disabled via headers. Disable callbacks from headers: None
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.responses_id_security.ResponsesIDSecurity object at 0x7ff1fe7aa270> is disabled via headers. Disable callbacks from headers: None
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x7ff1fe7aa3c0> is disabled via headers. Disable callbacks from headers: None
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m19:32:22 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7ff1ff2f1ba0> is disabled via headers. Disable callbacks from headers: None
INFO:     127.0.0.1:56078 - "POST /chat/completions HTTP/1.1" 200 OK
INFO:     Shutting down
INFO:     Waiting for application shutdown.
[92m19:32:22 - LiteLLM Proxy:INFO[0m: proxy_server.py:794 - SESSION REUSE: Closed shared aiohttp session
[92m19:32:22 - LiteLLM Proxy:INFO[0m: proxy_server.py:608 - Shutting down LiteLLM Proxy Server
INFO:     Application shutdown complete.
INFO:     Finished server process [2777]
